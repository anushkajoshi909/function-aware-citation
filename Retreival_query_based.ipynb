{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- EDIT THESE PATHS IF NEEDED ----\n",
    "jsonl_files = [\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0101_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0102_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0103_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0104_001.jsonl\",\n",
    "]\n",
    "\n",
    "# Directory where we'll save the index + metadata\n",
    "out_dir = Path(\"e5_index_subset_1\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model to use: 'intfloat/e5-base-v2' is a good speed/quality trade-off\n",
    "E5_MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "\n",
    "# Limit (optional): set to None to index everything\n",
    "MAX_PAPERS = None  # e.g., 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in 6.7 s\n",
      "Warmup ok\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”’ Force CPU + tame threads + use local cache to avoid any network / CUDA shenanigans\n",
    "import os, torch, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"          # force no-GPU path\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # quieter + safer in notebooks\n",
    "os.environ[\"HF_HOME\"] = \"./hf_cache\"             # local cache (no network)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "E5_MODEL_NAME = \"intfloat/e5-small-v2\"  # swap to e5-base-v2 later\n",
    "\n",
    "t0 = time.time()\n",
    "model = SentenceTransformer(E5_MODEL_NAME, device=\"cpu\", cache_folder=\"./hf_cache\")\n",
    "print(\"Loaded model in\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# warmup to avoid first-call lag\n",
    "_ = model.encode([\"query: warmup\"], normalize_embeddings=True)\n",
    "print(\"Warmup ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading arXiv_src_0101_001.jsonl: 2319it [00:02, 868.61it/s] \n",
      "Reading arXiv_src_0102_001.jsonl: 2192it [00:02, 925.20it/s] \n",
      "Reading arXiv_src_0103_001.jsonl: 2441it [00:02, 981.49it/s] \n",
      "Reading arXiv_src_0104_001.jsonl: 2310it [00:02, 844.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9262 unique papers\n",
      "           paper_id                                              title  \\\n",
      "0  quant-ph/0101147               Radiation trapping in coherent media   \n",
      "1  quant-ph/0101145  Mimicking a Kerrlike medium in the dispersive ...   \n",
      "2  quant-ph/0101144  What is Possible Without Disturbing Partially ...   \n",
      "\n",
      "                                             authors  year  \n",
      "0  [A. B. Matsko, I. Novikova, M. O. Scully, G. R...  2001  \n",
      "1     [A. B. Klimov, L. L. Sanchez-Soto, J. Delgado]  2001  \n",
      "2                    [Masato Koashi, Nobuyuki Imoto]  2001  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build_meta_with_authors.py (cell)\n",
    "import json, ast, re, os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "MAX_PAPERS = None  # or an int to truncate during dev\n",
    "\n",
    "_yr_re = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def best_year_from_obj(obj):\n",
    "    for key in (\"year\",\"published\",\"date\",\"update_date\",\"created\"):\n",
    "        if key in obj and obj[key]:\n",
    "            s = str(obj[key])\n",
    "            try:\n",
    "                y = int(s[:4])\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "            except Exception:\n",
    "                m = _yr_re.search(s)\n",
    "                if m: return int(m.group(0))\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    pid = obj.get(\"paper_id\") or md.get(\"id\") or obj.get(\"id\") or md.get(\"arxiv_id\") or obj.get(\"arxiv_id\")\n",
    "    if isinstance(pid, str) and \"/\" in pid:\n",
    "        try:\n",
    "            yy = int(pid.split(\"/\")[1][:2])\n",
    "            return 2000 + yy if yy < 50 else 1900 + yy\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def extract_title_abstract(obj):\n",
    "    title = None\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    title = md.get(\"title\") or obj.get(\"title\")\n",
    "    abstract = None\n",
    "    if isinstance(md.get(\"abstract\"), str):\n",
    "        abstract = md[\"abstract\"]\n",
    "    if not abstract and isinstance(obj.get(\"abstract\"), dict):\n",
    "        abstract = obj[\"abstract\"].get(\"text\")\n",
    "    if not abstract:\n",
    "        abstract = obj.get(\"abstract\")\n",
    "    return title, abstract\n",
    "\n",
    "def norm_raw_authors(raw):\n",
    "    if raw is None: return []\n",
    "    if isinstance(raw, (list, tuple)): return [str(x).strip() for x in raw if str(x).strip()]\n",
    "    s = str(raw).strip()\n",
    "    if not s: return []\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try: data = json.loads(s)\n",
    "        except Exception:\n",
    "            try: data = ast.literal_eval(s)\n",
    "            except Exception: data = None\n",
    "        if isinstance(data, list): return norm_raw_authors(data)\n",
    "    sep = \";\" if \";\" in s else \",\"\n",
    "    return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "\n",
    "def authors_from_parsed(ap):\n",
    "    out=[]\n",
    "    if isinstance(ap, list):\n",
    "        for it in ap:\n",
    "            if isinstance(it, dict):\n",
    "                nm=(\" \".join([it.get(\"first\",\"\"), it.get(\"last\",\"\")])).strip()\n",
    "            elif isinstance(it, (list,tuple)):\n",
    "                last=str(it[0]).strip() if len(it)>0 else \"\"\n",
    "                first=str(it[1]).strip() if len(it)>1 else \"\"\n",
    "                nm=(\" \".join([first,last])).strip()\n",
    "            else:\n",
    "                nm=str(it).strip()\n",
    "            if nm: out.append(nm)\n",
    "    return out\n",
    "\n",
    "def authors_from_obj(obj):\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    if \"authors_parsed\" in obj:\n",
    "        a = authors_from_parsed(obj[\"authors_parsed\"])\n",
    "        if a: return a\n",
    "    if \"authors_parsed\" in md:\n",
    "        a = authors_from_parsed(md[\"authors_parsed\"])\n",
    "        if a: return a\n",
    "    if \"authors\" in obj:\n",
    "        a = norm_raw_authors(obj[\"authors\"])\n",
    "        if a: return a\n",
    "    if \"authors\" in md:\n",
    "        a = norm_raw_authors(md[\"authors\"])\n",
    "        if a: return a\n",
    "    return []\n",
    "\n",
    "def get_pid(obj):\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    return obj.get(\"paper_id\") or md.get(\"id\") or obj.get(\"id\") or md.get(\"arxiv_id\") or obj.get(\"arxiv_id\")\n",
    "\n",
    "rows = []\n",
    "for path in jsonl_files:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Reading {os.path.basename(path)}\"):\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            pid = get_pid(obj)\n",
    "            title, abstract = extract_title_abstract(obj)\n",
    "            if not pid or not title or not abstract:\n",
    "                continue\n",
    "            authors = authors_from_obj(obj)\n",
    "            year = best_year_from_obj(obj)\n",
    "            rows.append({\n",
    "                \"paper_id\": pid,\n",
    "                \"title\": title.strip(),\n",
    "                \"abstract\": str(abstract).strip(),\n",
    "                \"authors\": authors,\n",
    "                \"year\": year\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df.drop_duplicates(subset=[\"paper_id\"]).reset_index(drop=True)\n",
    "if MAX_PAPERS: df = df.head(MAX_PAPERS)\n",
    "print(f\"Loaded {len(df)} unique papers\")\n",
    "print(df.head(3)[[\"paper_id\",\"title\",\"authors\",\"year\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Reload index & metadata\n",
    "index = faiss.read_index(str(out_dir / \"index.faiss\"))\n",
    "meta  = pd.read_parquet(out_dir / \"meta.parquet\")\n",
    "\n",
    "# âœ… Reuse the already-loaded model from earlier\n",
    "q_model = model                      # <-- do NOT call SentenceTransformer() again\n",
    "_ = q_model.encode([\"query: warmup\"], normalize_embeddings=True)  # quick warmup\n",
    "\n",
    "def encode_query(q: str):\n",
    "    return q_model.encode([f\"query: {q}\"], normalize_embeddings=True).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# helpers (format/display)\n",
    "# -----------------------\n",
    "\n",
    "def _trim(text, max_chars=450):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = str(text).strip()\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    cut = s[:max_chars].rsplit(\" \", 1)[0]\n",
    "    return cut + \"â€¦\"\n",
    "\n",
    "\n",
    "def _format_authors(a, k=3):\n",
    "    if a is None:\n",
    "        return []\n",
    "    if isinstance(a, (list, tuple)):\n",
    "        names = [str(x).strip() for x in a if str(x).strip()]\n",
    "    else:\n",
    "        sep = \";\" if \";\" in str(a) else \",\"\n",
    "        names = [t.strip() for t in str(a).split(sep) if t.strip()]\n",
    "    if not names:\n",
    "        return []\n",
    "    return names[:k] + ([\"et al.\"] if len(names) > k else [])\n",
    "\n",
    "\n",
    "_yr_re = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def _best_year(row):\n",
    "    \"\"\"Robustly extract a plausible year (int) from heterogeneous row fields without ambiguous truth checks.\"\"\"\n",
    "    def _first_scalar(x):\n",
    "        if isinstance(x, (list, tuple, np.ndarray, pd.Series)):\n",
    "            return x[0] if len(x) > 0 else None\n",
    "        return x\n",
    "\n",
    "    for key in (\"year\", \"published\", \"date\", \"update_date\", \"created\"):\n",
    "        if key in row:\n",
    "            val = _first_scalar(row.get(key))\n",
    "            if val is None:\n",
    "                continue\n",
    "            s = str(val)\n",
    "            # try first 4 chars\n",
    "            try:\n",
    "                y = int(s[:4])\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "            except Exception:\n",
    "                pass\n",
    "            # regex fallback anywhere in the string\n",
    "            m = _yr_re.search(s)\n",
    "            if m:\n",
    "                y = int(m.group(0))\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "\n",
    "    # last resort: parse from paper_id-like strings\n",
    "    pid = row.get(\"paper_id\") or row.get(\"arxiv_id\") or row.get(\"id\")\n",
    "    if isinstance(pid, str) and \"/\" in pid:\n",
    "        try:\n",
    "            yy = int(pid.split(\"/\")[1][:2])\n",
    "            return 2000 + yy if yy < 50 else 1900 + yy\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _extract_abstract(abstract_field):\n",
    "    if abstract_field is None:\n",
    "        return \"\"\n",
    "    if isinstance(abstract_field, dict):\n",
    "        return str(abstract_field.get(\"text\") or abstract_field.get(\"abstract\") or \"\")\n",
    "    return str(abstract_field)\n",
    "\n",
    "# -----------------------\n",
    "# stopwords + token utils\n",
    "# -----------------------\n",
    "\n",
    "DEFAULT_STOPWORDS = {\n",
    "    \"a\",\"an\",\"and\",\"the\",\"of\",\"to\",\"in\",\"on\",\"for\",\"with\",\"by\",\"as\",\"at\",\"or\",\"but\",\"if\",\"than\",\"then\",\n",
    "    \"from\",\"into\",\"over\",\"under\",\"between\",\"within\",\"without\",\"about\",\"via\",\"per\",\"through\",\"across\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"do\",\"does\",\"did\",\"can\",\"could\",\n",
    "    \"may\",\"might\",\"will\",\"would\",\"shall\",\"should\",\"must\",\"not\",\"no\",\"nor\",\"also\",\"both\",\"either\",\"neither\",\n",
    "    \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"their\",\"our\",\"your\",\"his\",\"her\",\"them\",\"they\",\"we\",\"you\",\"i\",\n",
    "    \"such\",\"thus\",\"there\",\"here\",\"where\",\"when\",\"which\",\"who\",\"whom\",\"whose\",\"what\",\"why\",\"how\",\n",
    "    \"using\",\"use\",\"used\",\"based\",\"approach\",\"approaches\",\"method\",\"methods\",\"result\",\"results\",\"show\",\n",
    "    \"shows\",\"shown\",\"paper\",\"study\",\"work\",\"new\"\n",
    "}\n",
    "\n",
    "def minmax_norm(x):\n",
    "    x = np.asarray(x, dtype=np.float32).reshape(-1)\n",
    "    if x.size == 0:\n",
    "        return x\n",
    "    lo, hi = float(np.min(x)), float(np.max(x))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or (hi - lo) < 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - lo) / (hi - lo)\n",
    "\n",
    "_token_re = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return [t.lower() for t in _token_re.findall(str(text))]\n",
    "\n",
    "def content_terms(tokens, stopwords, min_len=3):\n",
    "    return [t for t in tokens if not t.isdigit() and len(t) >= min_len and t not in stopwords]\n",
    "\n",
    "# -----------------------\n",
    "# robust author extraction\n",
    "# -----------------------\n",
    "\n",
    "def _authors_from_row(row):\n",
    "\n",
    "    def _norm_raw_authors(raw):\n",
    "        if raw is None:\n",
    "            return []\n",
    "        if isinstance(raw, (list, tuple)):\n",
    "            return [str(x).strip() for x in raw if str(x).strip()]\n",
    "        s = str(raw).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                data = json.loads(s)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "            if isinstance(data, list):\n",
    "                return _norm_raw_authors(data)\n",
    "        sep = \";\" if \";\" in s else \",\"\n",
    "        return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "\n",
    "    def _norm_authors_parsed(ap):\n",
    "        out = []\n",
    "        if isinstance(ap, (list, tuple)):\n",
    "            for item in ap:\n",
    "                if isinstance(item, (list, tuple)):\n",
    "                    last = str(item[0]).strip() if len(item) > 0 else \"\"\n",
    "                    first = str(item[1]).strip() if len(item) > 1 else \"\"\n",
    "                    name = \" \".join([first, last]).strip()\n",
    "                    if name:\n",
    "                        out.append(name)\n",
    "                elif isinstance(item, dict):\n",
    "                    first = str(item.get(\"first\", \"\")).strip()\n",
    "                    last = str(item.get(\"last\", \"\")).strip()\n",
    "                    name = \" \".join([first, last]).strip()\n",
    "                    if name:\n",
    "                        out.append(name)\n",
    "                else:\n",
    "                    s = str(item).strip()\n",
    "                    if s:\n",
    "                        out.append(s)\n",
    "        elif isinstance(ap, str) and ap.strip():\n",
    "            try:\n",
    "                data = json.loads(ap)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(ap)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "            if isinstance(data, list):\n",
    "                return _norm_authors_parsed(data)\n",
    "        return out\n",
    "\n",
    "    # avoid pd.notna() on non-scalars â€” just check for presence/non-empty\n",
    "    if \"authors\" in row:\n",
    "        names = _norm_raw_authors(row.get(\"authors\"))\n",
    "        if names:\n",
    "            return names\n",
    "\n",
    "    if \"authors_parsed\" in row:\n",
    "        names = _norm_authors_parsed(row.get(\"authors_parsed\"))\n",
    "        if names:\n",
    "            return names\n",
    "\n",
    "    # nested metadata dict (JSONL)\n",
    "    if \"metadata\" in row and isinstance(row.get(\"metadata\"), dict):\n",
    "        md = row[\"metadata\"]\n",
    "        if \"authors\" in md:\n",
    "            names = _norm_raw_authors(md.get(\"authors\"))\n",
    "            if names:\n",
    "                return names\n",
    "        if \"authors_parsed\" in md:\n",
    "            names = _norm_authors_parsed(md.get(\"authors_parsed\"))\n",
    "            if names:\n",
    "                return names\n",
    "\n",
    "    return []\n",
    "\n",
    "# -----------------------\n",
    "# meta building from JSONL\n",
    "# -----------------------\n",
    "\n",
    "def _rows_from_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            md = obj.get(\"metadata\", {}) or {}\n",
    "            authors_field = md.get(\"authors\") or obj.get(\"authors\")\n",
    "            yield {\n",
    "                \"paper_id\": obj.get(\"paper_id\") or md.get(\"id\"),\n",
    "                \"title\": md.get(\"title\", \"\") or obj.get(\"title\", \"\"),\n",
    "                \"abstract\": obj.get(\"abstract\", md.get(\"abstract\", \"\")),\n",
    "                \"authors\": authors_field,\n",
    "                \"metadata\": md,\n",
    "            }\n",
    "\n",
    "def build_meta_from_jsonl(paths):\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "    all_rows = []\n",
    "    for p in paths:\n",
    "        for row in _rows_from_jsonl(p):\n",
    "            all_rows.append(row)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if \"paper_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"paper_id\"], keep=\"first\")\n",
    "    if \"title\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# main search function\n",
    "# -----------------------\n",
    "\n",
    "# COSINE-ONLY retrieval (query-only)\n",
    "def search_post_filter(\n",
    "    query,\n",
    "    topN=20,\n",
    "    topK_return=10,\n",
    "    normalize_scores=False,\n",
    "    stopwords=None,\n",
    "    min_term_len=3,\n",
    "    abstract_chars=450,\n",
    "    authors_shown=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Requires globals: index (faiss index), meta (DataFrame aligned to index), encode_query (-> np.float32 [1,d])\n",
    "    \"\"\"\n",
    "    stopwords = DEFAULT_STOPWORDS if stopwords is None else set(stopwords)\n",
    "\n",
    "    # 1) semantic retrieve\n",
    "    qv = encode_query(query)  # shape (1, d)\n",
    "    scores, idxs = index.search(qv, int(topN))  # shapes (1, topN)\n",
    "    scores, idxs = scores[0].astype(np.float32), idxs[0]\n",
    "\n",
    "    # 2) order purely by cosine\n",
    "    order = np.argsort(-scores)\n",
    "    display_scores = minmax_norm(scores) if normalize_scores else scores\n",
    "\n",
    "    # 3) lexical explainers\n",
    "    q_terms_all  = tokenize(query)\n",
    "    q_terms_used = content_terms(q_terms_all, stopwords, min_len=min_term_len)\n",
    "\n",
    "    out = []\n",
    "    limit = min(int(topK_return), len(order))\n",
    "    for rank_pos in range(limit):\n",
    "        r = order[rank_pos]\n",
    "        if r < 0 or r >= len(idxs):\n",
    "            continue  # safety\n",
    "        row = meta.iloc[idxs[r]]\n",
    "\n",
    "        title = row.get(\"title\", \"\")\n",
    "        abstract_txt = _extract_abstract(row.get(\"abstract\", \"\"))\n",
    "        paper_id = row.get(\"paper_id\") or row.get(\"arxiv_id\") or row.get(\"id\")\n",
    "\n",
    "        title_tokens = content_terms(tokenize(title), stopwords, min_len=min_term_len)\n",
    "        abs_tokens   = content_terms(tokenize(abstract_txt), stopwords, min_len=min_term_len)\n",
    "\n",
    "        title_matches = sorted(set(q_terms_used) & set(title_tokens))\n",
    "        abs_matches   = sorted(set(q_terms_used) & set(abs_tokens))\n",
    "\n",
    "        authors_list = _authors_from_row(row)\n",
    "        authors_fmt  = _format_authors(authors_list, k=authors_shown)\n",
    "\n",
    "        out.append({\n",
    "            \"score\": float(display_scores[r]),\n",
    "            \"cosine\": float(scores[r]),\n",
    "            \"title\": title,\n",
    "            \"abstract\": _trim(abstract_txt, abstract_chars),  # preview\n",
    "            \"abstract_full\": abstract_txt,                    # full\n",
    "            \"arxiv_id\": paper_id,\n",
    "            \"year\": _best_year(row),\n",
    "            \"authors\": authors_fmt,\n",
    "            \"title_matches\": title_matches,\n",
    "            \"abs_matches\": abs_matches,\n",
    "            \"query_terms_used\": q_terms_used,\n",
    "            # removed 'function_requested' (no desired_function in signature anymore)\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# convenience: results -> DataFrame\n",
    "# -----------------------\n",
    "\n",
    "def to_df(res):\n",
    "    cols = [\"cosine\",\"title\",\"year\",\"arxiv_id\",\"authors\",\"title_matches\",\"abs_matches\"]\n",
    "    return pd.DataFrame([{k: r.get(k) for k in cols} for r in res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def _safe_join(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, list):\n",
    "        return \", \".join(str(t) for t in x)\n",
    "    return str(x)\n",
    "\n",
    "def display_results_table(results):\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            \"Title\": r.get(\"title\", \"\"),\n",
    "            \"Year\": r.get(\"year\", \"\"),\n",
    "            \"Authors\": _safe_join(r.get(\"authors\", [])),\n",
    "            \"Abstract\": r.get(\"abstract\", \"\"),  # already trimmed upstream\n",
    "            \"Title Matches\": _safe_join(r.get(\"title_matches\", [])),\n",
    "            \"Abstract Matches\": _safe_join(r.get(\"abs_matches\", [])),\n",
    "            \"Score (norm)\": r.get(\"score\", r.get(\"cosine\", None)),\n",
    "            \"Cosine (raw)\": r.get(\"cosine\", None),\n",
    "        } for r in (results or [])\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_current_classification(\n",
    "    classified_path=\"classified_outputs.jsonl\",\n",
    "    out_dir=\"outputs\",\n",
    "    topN=50,\n",
    "    topK_return=10,\n",
    "    normalize_scores=True,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Read the latest query from `classified_outputs.jsonl`, run query-only retrieval\n",
    "    via `search(...)`, and write top-k candidates to JSONL and CSV.\n",
    "\n",
    "    - No sentence-level loop\n",
    "    - No answer/explanations\n",
    "    - Prefers full abstract if present in results (abstract_full), else falls back to abstract\n",
    "    - Robust against numpy arrays to avoid: \"The truth value of an array with more than one element is ambiguous\"\n",
    "    \"\"\"\n",
    "    import os, json\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "\n",
    "    def read_last_jsonl(path: str):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        if not lines:\n",
    "            raise ValueError(f\"No lines found in {path}\")\n",
    "        try:\n",
    "            return json.loads(lines[-1])\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Last line is not valid JSON: {e}\\nLine: {lines[-1][:200]}...\")\n",
    "\n",
    "    def as_list(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        return [x]\n",
    "\n",
    "    def join_if_list(x, sep=\", \"):\n",
    "        if isinstance(x, list):\n",
    "            return sep.join(str(t) for t in x)\n",
    "        return \"\" if x is None else str(x)\n",
    "\n",
    "    def safe_year(y):\n",
    "        try:\n",
    "            # Avoid numpy types/arrays ambiguity\n",
    "            if isinstance(y, (list, tuple)):\n",
    "                y = y[0] if y else None\n",
    "            s = str(y).strip()\n",
    "            if not s:\n",
    "                return None\n",
    "            return int(s[:4])\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_jsonl = os.path.join(out_dir, \"topk_candidates_query.jsonl\")\n",
    "    out_csv   = os.path.join(out_dir, \"topk_candidates_query.csv\")\n",
    "\n",
    "    obj = read_last_jsonl(classified_path)\n",
    "\n",
    "    query = (obj.get(\"query\") or \"\").strip()\n",
    "    if not query:\n",
    "        raise ValueError(\"No 'query' found in the latest classified_outputs.jsonl entry.\")\n",
    "\n",
    "    # provenance of classifier labels (not used for ranking)\n",
    "    cls = obj.get(\"citation_function_classification\") or {}\n",
    "    cls_funcs = cls.get(\"citation_functions\") or []\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[debug] query: {query}\")\n",
    "        if cls_funcs:\n",
    "            print(f\"[debug] classifier labels: {cls_funcs}\")\n",
    "\n",
    "    retrieved_at = datetime.utcnow().isoformat()\n",
    "\n",
    "    # ---- Query-only retrieval (use search; fall back to search_post_filter if needed) ----\n",
    "    retrieval_fn = globals().get(\"search\")\n",
    "    if retrieval_fn is None:\n",
    "        retrieval_fn = globals().get(\"search_post_filter\")\n",
    "    if retrieval_fn is None:\n",
    "        raise RuntimeError(\"Neither 'search' nor 'search_post_filter' is defined in the current scope.\")\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        # Ensure we always get a Python list (not a numpy array)\n",
    "        res = retrieval_fn(\n",
    "            query=query,\n",
    "            topN=int(topN),\n",
    "            topK_return=int(topK_return),\n",
    "            normalize_scores=bool(normalize_scores)\n",
    "        )\n",
    "        if isinstance(res, list):\n",
    "            results = res\n",
    "        elif res is None:\n",
    "            results = []\n",
    "        else:\n",
    "            # Defensive: convert iterables to list\n",
    "            try:\n",
    "                results = list(res)\n",
    "            except Exception:\n",
    "                results = []\n",
    "        if debug:\n",
    "            print(f\"[debug] got {len(results)} candidates\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"[debug] retrieval error: {e}\")\n",
    "        results = []\n",
    "\n",
    "    # ---- Flatten results for export ----\n",
    "    rows = []\n",
    "    for rank, r in enumerate(results):\n",
    "        # prefer full abstract if present in retrieval output\n",
    "        abs_full = r.get(\"abstract_full\")\n",
    "        if abs_full is None or not str(abs_full).strip():\n",
    "            abs_full = r.get(\"abstract\", \"\")  # fallback\n",
    "\n",
    "        rows.append({\n",
    "            \"rank\": int(rank),\n",
    "            \"paper_id\": r.get(\"arxiv_id\") or r.get(\"paper_id\") or \"\",\n",
    "            \"title\": r.get(\"title\", \"\"),\n",
    "            \"year\": safe_year(r.get(\"year\")),\n",
    "            \"authors\": join_if_list(r.get(\"authors\")),\n",
    "            \"abstract\": str(abs_full or \"\"),\n",
    "            \"score\": (float(r.get(\"score\")) if r.get(\"score\") is not None else None),   # normalized if provided\n",
    "            \"cosine\": (float(r.get(\"cosine\")) if r.get(\"cosine\") is not None else None),\n",
    "            \"title_matches\": join_if_list(r.get(\"title_matches\")),\n",
    "            \"abs_matches\": join_if_list(r.get(\"abs_matches\")),\n",
    "            \"query_terms_used\": join_if_list(r.get(\"query_terms_used\")),\n",
    "            \"classifier_functions\": join_if_list(as_list(cls_funcs)),\n",
    "            \"retrieval_error\": None,\n",
    "            \"retrieved_at\": retrieved_at\n",
    "        })\n",
    "\n",
    "    # If no results, emit a stub row so downstream doesnâ€™t break\n",
    "    if len(rows) == 0:\n",
    "        rows.append({\n",
    "            \"rank\": None,\n",
    "            \"paper_id\": \"\",\n",
    "            \"title\": \"\",\n",
    "            \"year\": None,\n",
    "            \"authors\": \"\",\n",
    "            \"abstract\": \"\",\n",
    "            \"score\": None,\n",
    "            \"cosine\": None,\n",
    "            \"title_matches\": \"\",\n",
    "            \"abs_matches\": \"\",\n",
    "            \"query_terms_used\": \"\",\n",
    "            \"classifier_functions\": join_if_list(as_list(cls_funcs)),\n",
    "            \"retrieval_error\": \"no_results\",\n",
    "            \"retrieved_at\": retrieved_at\n",
    "        })\n",
    "\n",
    "    # ---- Write outputs (overwrite each run) ----\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in rows:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"rows written: {len(rows)}\")\n",
    "        print(f\"- {out_jsonl}\\n- {out_csv}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] query: How might future research compare the low-energy effective action of the Nambu-Goldstone boson in this model to other theoretical frameworks?\n",
      "[debug] classifier labels: ['Compares', 'FutureWork']\n",
      "[debug] got 20 candidates\n",
      "rows written: 20\n",
      "- outputs/topk_candidates_query.jsonl\n",
      "- outputs/topk_candidates_query.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>cosine</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title_matches</th>\n",
       "      <th>abs_matches</th>\n",
       "      <th>query_terms_used</th>\n",
       "      <th>retrieved_at</th>\n",
       "      <th>classifier_functions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863618</td>\n",
       "      <td>The low energy limit of the non-commutative We...</td>\n",
       "      <td>2001</td>\n",
       "      <td>H. O. GirottiMarcelo GomesA. Y. PetrovVictor O...</td>\n",
       "      <td>hep-th/0101159</td>\n",
       "      <td>energy, low, model</td>\n",
       "      <td>action, boson, effective, energy, low, model</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.931730</td>\n",
       "      <td>0.862003</td>\n",
       "      <td>Spontaneous Lorentz Symmetry Breaking by Anti-...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Kiyoshi HigashijimaNaoto Yokoi</td>\n",
       "      <td>hep-th/0101222</td>\n",
       "      <td></td>\n",
       "      <td>action, boson, effective, energy, goldstone, l...</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.843280</td>\n",
       "      <td>0.859911</td>\n",
       "      <td>Masses of the pseudo-Nambu-Goldstone bosons in...</td>\n",
       "      <td>2001</td>\n",
       "      <td>V. A. MiranskyI. A. ShovkovyL. C. R. Wijewardhana</td>\n",
       "      <td>hep-ph/0104194</td>\n",
       "      <td>goldstone, nambu</td>\n",
       "      <td>action, boson, effective, energy, goldstone, n...</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.576787</td>\n",
       "      <td>0.853608</td>\n",
       "      <td>The 2-site Hubbard and t-J models</td>\n",
       "      <td>2001</td>\n",
       "      <td>Adolfo AvellaFerdinando ManciniTaiichiro Saikawa</td>\n",
       "      <td>cond-mat/0103610</td>\n",
       "      <td></td>\n",
       "      <td>energy, low, model</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.569419</td>\n",
       "      <td>0.853433</td>\n",
       "      <td>The $\\eta NN$-system at low energy within a th...</td>\n",
       "      <td>2001</td>\n",
       "      <td>A. FixH. Arenhoevel</td>\n",
       "      <td>nucl-th/0104032</td>\n",
       "      <td>energy, low</td>\n",
       "      <td>energy, low, model</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.551520</td>\n",
       "      <td>0.853010</td>\n",
       "      <td>Quantum stabilization of thermal brane-worlds ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Shin'ichi NojiriSergei D. OdintsovSachiko Ogushi</td>\n",
       "      <td>hep-th/0102082</td>\n",
       "      <td></td>\n",
       "      <td>effective, energy, low</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.521329</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>Model-Independent Analysis of g_\\mu-2</td>\n",
       "      <td>2001</td>\n",
       "      <td>Martin B. EinhornJ. Wudka</td>\n",
       "      <td>hep-ph/0103034</td>\n",
       "      <td>model</td>\n",
       "      <td>boson, effective, model, other</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.852147</td>\n",
       "      <td>On the fourth P_11 resonance predicted by the ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>L. TheuÃŸlR. F. Wagenbrunn</td>\n",
       "      <td>nucl-th/0104024</td>\n",
       "      <td>model</td>\n",
       "      <td>boson, model</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.483704</td>\n",
       "      <td>0.851406</td>\n",
       "      <td>Two leg quantum Ising ladder: A bosonization s...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Dave AllenPatrick AzariaPhilippe Lecheminant</td>\n",
       "      <td>cond-mat/0101404</td>\n",
       "      <td>model</td>\n",
       "      <td>effective, energy, low, model</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.406883</td>\n",
       "      <td>0.849589</td>\n",
       "      <td>Low-Energy Supersymmetry and its Phenomenology</td>\n",
       "      <td>2001</td>\n",
       "      <td>Howard E. Haber</td>\n",
       "      <td>hep-ph/0103095</td>\n",
       "      <td>energy, low</td>\n",
       "      <td>energy, future, low, model</td>\n",
       "      <td>future, research, compare, low, energy, effect...</td>\n",
       "      <td>2025-09-14T17:03:58.665275</td>\n",
       "      <td>Compares, FutureWork</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank     score    cosine  \\\n",
       "0     0  1.000000  0.863618   \n",
       "1     1  0.931730  0.862003   \n",
       "2     2  0.843280  0.859911   \n",
       "3     3  0.576787  0.853608   \n",
       "4     4  0.569419  0.853433   \n",
       "5     5  0.551520  0.853010   \n",
       "6     6  0.521329  0.852296   \n",
       "7     7  0.515024  0.852147   \n",
       "8     8  0.483704  0.851406   \n",
       "9     9  0.406883  0.849589   \n",
       "\n",
       "                                               title  year  \\\n",
       "0  The low energy limit of the non-commutative We...  2001   \n",
       "1  Spontaneous Lorentz Symmetry Breaking by Anti-...  2001   \n",
       "2  Masses of the pseudo-Nambu-Goldstone bosons in...  2001   \n",
       "3                  The 2-site Hubbard and t-J models  2001   \n",
       "4  The $\\eta NN$-system at low energy within a th...  2001   \n",
       "5  Quantum stabilization of thermal brane-worlds ...  2001   \n",
       "6              Model-Independent Analysis of g_\\mu-2  2001   \n",
       "7  On the fourth P_11 resonance predicted by the ...  2001   \n",
       "8  Two leg quantum Ising ladder: A bosonization s...  2001   \n",
       "9     Low-Energy Supersymmetry and its Phenomenology  2001   \n",
       "\n",
       "                                             authors          paper_id  \\\n",
       "0  H. O. GirottiMarcelo GomesA. Y. PetrovVictor O...    hep-th/0101159   \n",
       "1                     Kiyoshi HigashijimaNaoto Yokoi    hep-th/0101222   \n",
       "2  V. A. MiranskyI. A. ShovkovyL. C. R. Wijewardhana    hep-ph/0104194   \n",
       "3   Adolfo AvellaFerdinando ManciniTaiichiro Saikawa  cond-mat/0103610   \n",
       "4                                A. FixH. Arenhoevel   nucl-th/0104032   \n",
       "5   Shin'ichi NojiriSergei D. OdintsovSachiko Ogushi    hep-th/0102082   \n",
       "6                          Martin B. EinhornJ. Wudka    hep-ph/0103034   \n",
       "7                          L. TheuÃŸlR. F. Wagenbrunn   nucl-th/0104024   \n",
       "8       Dave AllenPatrick AzariaPhilippe Lecheminant  cond-mat/0101404   \n",
       "9                                    Howard E. Haber    hep-ph/0103095   \n",
       "\n",
       "        title_matches                                        abs_matches  \\\n",
       "0  energy, low, model       action, boson, effective, energy, low, model   \n",
       "1                      action, boson, effective, energy, goldstone, l...   \n",
       "2    goldstone, nambu  action, boson, effective, energy, goldstone, n...   \n",
       "3                                                     energy, low, model   \n",
       "4         energy, low                                 energy, low, model   \n",
       "5                                                 effective, energy, low   \n",
       "6               model                     boson, effective, model, other   \n",
       "7               model                                       boson, model   \n",
       "8               model                      effective, energy, low, model   \n",
       "9         energy, low                         energy, future, low, model   \n",
       "\n",
       "                                    query_terms_used  \\\n",
       "0  future, research, compare, low, energy, effect...   \n",
       "1  future, research, compare, low, energy, effect...   \n",
       "2  future, research, compare, low, energy, effect...   \n",
       "3  future, research, compare, low, energy, effect...   \n",
       "4  future, research, compare, low, energy, effect...   \n",
       "5  future, research, compare, low, energy, effect...   \n",
       "6  future, research, compare, low, energy, effect...   \n",
       "7  future, research, compare, low, energy, effect...   \n",
       "8  future, research, compare, low, energy, effect...   \n",
       "9  future, research, compare, low, energy, effect...   \n",
       "\n",
       "                 retrieved_at  classifier_functions  \n",
       "0  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "1  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "2  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "3  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "4  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "5  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "6  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "7  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "8  2025-09-14T17:03:58.665275  Compares, FutureWork  \n",
       "9  2025-09-14T17:03:58.665275  Compares, FutureWork  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Run llm_test.py to regenerate the file (it overwrites classified_outputs.jsonl)\n",
    "\n",
    "# 2) Process the latest query (query-only retrieval)\n",
    "df_all = process_current_classification(  # <- use the query-only function\n",
    "    classified_path=\"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/classified_outputs.jsonl\",\n",
    "    out_dir=\"outputs\",\n",
    "    topN=50,\n",
    "    topK_return=20,\n",
    "    normalize_scores=True,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "# 3) Inspect results (no sentence_idx anymore)\n",
    "if df_all.empty:\n",
    "    print(\"No rows saved â€” check your classification or retrieval.\")\n",
    "else:\n",
    "    cols = [\"rank\",\"score\",\"cosine\",\"title\",\"year\",\"authors\",\"paper_id\",\"title_matches\",\"abs_matches\",\"query_terms_used\",\"retrieved_at\",\"classifier_functions\"]\n",
    "    cols = [c for c in cols if c in df_all.columns]  # keep only existing\n",
    "    display(df_all[df_all[\"rank\"].notna()].sort_values(\"rank\")[cols].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cosine                                              title  year  \\\n",
      "0  0.828061      Microscopic Reaction Dynamics at SPS and RHIC  2001   \n",
      "1  0.827704  Jet Quenching and the p-bar >= pi- Anomaly at ...  2001   \n",
      "2  0.826666      New results on the temporal structure of GRBs  2001   \n",
      "3  0.826383  On the mean field treatment of attractive inte...  2001   \n",
      "4  0.825162  Differential Cross Sections Measurement for th...  2001   \n",
      "5  0.825125  A contiuum model for low temperature relaxatio...  2001   \n",
      "6  0.824200  A semi-analytical approach to non-linear shock...  2001   \n",
      "7  0.823486  Vibrational origin of the fast relaxation proc...  2001   \n",
      "8  0.822597                                           1D Aging  2001   \n",
      "9  0.822466  On Echo Outbursts and ER UMa Supercycles in SU...  2001   \n",
      "\n",
      "           arxiv_id                                            authors  \\\n",
      "0   nucl-th/0104040                                  [Steffen A. Bass]   \n",
      "1   nucl-th/0104066                        [Ivan VitevMiklos Gyulassy]   \n",
      "2  astro-ph/0103011                                 [E. NakarT. Piran]   \n",
      "3  cond-mat/0104317                       [Kirill KatsovJohn D. Weeks]   \n",
      "4   nucl-ex/0101001  [M. BetigeriJ. BojowaldA. BudzanowskiA . Chatt...   \n",
      "5  cond-mat/0104235                                  [O. Pierre-Louis]   \n",
      "6  astro-ph/0104064                                   [Pasquale Blasi]   \n",
      "7  cond-mat/0104265                       [S. MossaG. MonacoG. Ruocco]   \n",
      "8  cond-mat/0103494      [L. R. FontesM. IsopiC. M. NewmanD. L. Stein]   \n",
      "9  astro-ph/0101102                                     [Coel Hellier]   \n",
      "\n",
      "  title_matches  abs_matches  \n",
      "0            []     [models]  \n",
      "1            []  [reactions]  \n",
      "2            []           []  \n",
      "3      [simple]     [simple]  \n",
      "4            []           []  \n",
      "5            []           []  \n",
      "6            []     [simple]  \n",
      "7            []           []  \n",
      "8            []     [quench]  \n",
      "9            []           []  \n"
     ]
    }
   ],
   "source": [
    "# Skip this block\n",
    "# # force authors into strings\n",
    "meta[\"authors\"] = meta[\"authors\"].apply(lambda x: x if isinstance(x, str) or x is None else str(x))\n",
    "\n",
    "# Example: one clause from a generated answer\n",
    "clause = \"Compared to simple quench models, back reactions temper and delay amplification.\"\n",
    "\n",
    "results = search(\n",
    "    query=clause,\n",
    "    topN=50,\n",
    "    topK_return=10\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for inspection\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results[[\"cosine\",\"title\",\"year\",\"arxiv_id\",\"authors\",\"title_matches\",\"abs_matches\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
