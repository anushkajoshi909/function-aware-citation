{"rank": 0, "score": 0.918131947517395, "paper_id": "astro-ph/0104152", "title": "An efficient parallel tree-code for the simulation of self-gravitating\n  systems", "abstract": "We describe a parallel version of our tree-code for the simulation of\nself-gravitating systems in Astrophysics. It is based on a dynamic and adaptive\nmethod for the domain decomposition, which exploits the hierarchical data\narrangement used by the tree-code. It shows low computational costs for the\nparallelization overhead -- less than 4% of the total CPU-time in the tests\ndone -- because the domain decomposition is performed 'on the fly' during the\ntree setting and the portion of the tree that is local to each processor\n'enriches' itself of remote data only when they are actually needed.\n  The performances of an implementation of the parallel code on a Cray T3E are\npresented and discussed. They exhibit a very good behaviour of the speedup (=15\nwith 16 processors and 10^5 particles) and a rather low load unbalancing (< 10%\nusing up to 16 processors), achieving a high computation speed in the forces\nevaluation (>10^4 particles/sec with 8 processors).", "abstract_full": "We describe a parallel version of our tree-code for the simulation of\nself-gravitating systems in Astrophysics. It is based on a dynamic and adaptive\nmethod for the domain decomposition, which exploits the hierarchical data\narrangement used by the tree-code. It shows low computational costs for the\nparallelization overhead -- less than 4% of the total CPU-time in the tests\ndone -- because the domain decomposition is performed 'on the fly' during the\ntree setting and the portion of the tree that is local to each processor\n'enriches' itself of remote data only when they are actually needed.\n  The performances of an implementation of the parallel code on a Cray T3E are\npresented and discussed. They exhibit a very good behaviour of the speedup (=15\nwith 16 processors and 10^5 particles) and a rather low load unbalancing (< 10%\nusing up to 16 processors), achieving a high computation speed in the forces\nevaluation (>10^4 particles/sec with 8 processors).", "authors": "P. Miocchi, R. Capuzzo-Dolcetta", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 1, "score": 0.8676350116729736, "paper_id": "astro-ph/0101447", "title": "An Integrated Procedure for Tree-Nbody Simulations: FLY and AstroMD", "abstract": "We present a code allowing to evolve three-dimensional self-gravitating\ncollisionless systems with a large number of particles N >= 10^7. FLY (Fast\nLevel-based N-bodY code) is a fully parallel code based on a tree algorithm. It\nadopts periodic boundary conditions implemented by means of the Ewald summation\ntechnique. FLY is based on the one-side communication paradigm and was\noriginally developed on a CRAY T3E system using the SHMEM library and it was\nported on SGI ORIGIN 2000 and on IBM SP. FLY (http://www.ct.astro.it/fly/)\nversion 1.1 is an open source freely available code. FLY data output can be\nanalysed with AstroMD, an analysis and visualization tool specifically designed\nto deal with the visualization and analysis of astrophysical data. AstroMD can\nmanage different physical quantities. It can find out structures without well\ndefined shape or symmetries, and perform quantitative calculations on selected\nregions. AstroMD (http://www.cineca.it/astromd) is a freely available code.", "abstract_full": "We present a code allowing to evolve three-dimensional self-gravitating\ncollisionless systems with a large number of particles N >= 10^7. FLY (Fast\nLevel-based N-bodY code) is a fully parallel code based on a tree algorithm. It\nadopts periodic boundary conditions implemented by means of the Ewald summation\ntechnique. FLY is based on the one-side communication paradigm and was\noriginally developed on a CRAY T3E system using the SHMEM library and it was\nported on SGI ORIGIN 2000 and on IBM SP. FLY (http://www.ct.astro.it/fly/)\nversion 1.1 is an open source freely available code. FLY data output can be\nanalysed with AstroMD, an analysis and visualization tool specifically designed\nto deal with the visualization and analysis of astrophysical data. AstroMD can\nmanage different physical quantities. It can find out structures without well\ndefined shape or symmetries, and perform quantitative calculations on selected\nregions. AstroMD (http://www.cineca.it/astromd) is a freely available code.", "authors": "U. Becciani, V. Antonuccio-Delogu, F. Buonomo, C. Gheller", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 2, "score": 0.8646338582038879, "paper_id": "astro-ph/0101148", "title": "Are You Ready to FLY in the Universe ? A Multi-platform N-body Tree Code\n  for Parallel Supercomputers", "abstract": "The code we describe (FLY) is a newly written code (using the tree N-body\nmethod), for three-dimensional self-gravitating collisionless systems\nevolution. FLY is a fully parallel code based on the tree Barnes-Hut algorithm\nand periodical boundary conditions are implemented by means of the Ewald\nsummation technique. FLY is based on the one-side communication paradigm to\nshare data among the processors, that access to remote private data avoiding\nany kind of synchronism. The code was originally developed on CRAY T3E system\nusing the logically SHared MEMory access routines (SHMEM) but it runs also on\nSGI ORIGIN systems and on IBM SP by using the Low-Level Application Programming\nInterface routines (LAPI). This new code reaches good performance in all\nsystems where it has been well-tested. This performance allows us today to\nconsider the code FLY among the most powerful parallel codes for tree N-body\nsimulations. The FLY version 1.1 is freely available on\nhttp://www.ct.astro.it/fly/ and it will be maintained and upgraded with new\nreleases.", "abstract_full": "The code we describe (FLY) is a newly written code (using the tree N-body\nmethod), for three-dimensional self-gravitating collisionless systems\nevolution. FLY is a fully parallel code based on the tree Barnes-Hut algorithm\nand periodical boundary conditions are implemented by means of the Ewald\nsummation technique. FLY is based on the one-side communication paradigm to\nshare data among the processors, that access to remote private data avoiding\nany kind of synchronism. The code was originally developed on CRAY T3E system\nusing the logically SHared MEMory access routines (SHMEM) but it runs also on\nSGI ORIGIN systems and on IBM SP by using the Low-Level Application Programming\nInterface routines (LAPI). This new code reaches good performance in all\nsystems where it has been well-tested. This performance allows us today to\nconsider the code FLY among the most powerful parallel codes for tree N-body\nsimulations. The FLY version 1.1 is freely available on\nhttp://www.ct.astro.it/fly/ and it will be maintained and upgraded with new\nreleases.", "authors": "U. Becciani, V. Antonuccio-Delogu", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 3, "score": 0.862087070941925, "paper_id": "astro-ph/0104258", "title": "Parallel Implementation of the PHOENIX Generalized Stellar Atmosphere\n  Program. III: A parallel algorithm for direct opacity sampling", "abstract": "We describe two parallel algorithms for line opacity calculations based on a\nlocal file and on a global file approach. The performance and scalability of\nboth approaches is discussed for different test cases and very different\nparallel computing systems. The results show that a global file approach is\nmore efficient on high-performance parallel supercomputers with dedicated\nparallel I/O subsystem whereas the local file approach is very useful on farms\nof workstations, e.g., cheap PC clusters.", "abstract_full": "We describe two parallel algorithms for line opacity calculations based on a\nlocal file and on a global file approach. The performance and scalability of\nboth approaches is discussed for different test cases and very different\nparallel computing systems. The results show that a global file approach is\nmore efficient on high-performance parallel supercomputers with dedicated\nparallel I/O subsystem whereas the local file approach is very useful on farms\nof workstations, e.g., cheap PC clusters.", "authors": "Peter H. Hauschildt, David K. Lowenthal, E. Baron", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 4, "score": 0.8565280437469482, "paper_id": "astro-ph/0103503", "title": "MLAPM - a C code for cosmological simulations", "abstract": "We present a computer code written in C that is designed to simulate\nstructure formation from collisionless matter. The code is purely grid-based\nand uses a recursively refined Cartesian grid to solve Poisson's equation for\nthe potential, rather than obtaining the potential from a Green's function.\nRefinements can have arbitrary shapes and in practice closely follow the\ncomplex morphology of the density field that evolves. The timestep shortens by\na factor two with each successive refinement. It is argued that an appropriate\nchoice of softening length is of great importance and that the softening should\nbe at all points an appropriate multiple of the local inter-particle\nseparation. Unlike tree and P3M codes, multigrid codes automatically satisfy\nthis requirement. We show that at early times and low densities in cosmological\nsimulations, the softening needs to be significantly smaller relative to the\ninter-particle separation than in virialized regions. Tests of the ability of\nthe code's Poisson solver to recover the gravitational fields of both\nvirialized halos and Zel'dovich waves are presented, as are tests of the code's\nability to reproduce analytic solutions for plane-wave evolution. The times\nrequired to conduct a LCDM cosmological simulation for various configurations\nare compared with the times required to complete the same simulation with the\nART, AP3M and GADGET codes. The power spectra, halo mass functions and\nhalo-halo correlation functions of simulations conducted with different codes\nare compared.", "abstract_full": "We present a computer code written in C that is designed to simulate\nstructure formation from collisionless matter. The code is purely grid-based\nand uses a recursively refined Cartesian grid to solve Poisson's equation for\nthe potential, rather than obtaining the potential from a Green's function.\nRefinements can have arbitrary shapes and in practice closely follow the\ncomplex morphology of the density field that evolves. The timestep shortens by\na factor two with each successive refinement. It is argued that an appropriate\nchoice of softening length is of great importance and that the softening should\nbe at all points an appropriate multiple of the local inter-particle\nseparation. Unlike tree and P3M codes, multigrid codes automatically satisfy\nthis requirement. We show that at early times and low densities in cosmological\nsimulations, the softening needs to be significantly smaller relative to the\ninter-particle separation than in virialized regions. Tests of the ability of\nthe code's Poisson solver to recover the gravitational fields of both\nvirialized halos and Zel'dovich waves are presented, as are tests of the code's\nability to reproduce analytic solutions for plane-wave evolution. The times\nrequired to conduct a LCDM cosmological simulation for various configurations\nare compared with the times required to complete the same simulation with the\nART, AP3M and GADGET codes. The power spectra, halo mass functions and\nhalo-halo correlation functions of simulations conducted with different codes\nare compared.", "authors": "Alexander Knebe, Andrew Green, James Binney", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 5, "score": 0.8550941348075867, "paper_id": "astro-ph/0102068", "title": "A 3D MHD model of astrophysical flows: algorithms, tests and\n  parallelisation", "abstract": "In this paper we describe a numerical method designed for modelling different\nkinds of astrophysical flows in three dimensions. Our method is a standard\nexplicit finite difference method employing the local shearing-box technique.\n  To model the features of astrophysical systems, which are usually\ncompressible, magnetised and turbulent, it is desirable to have high spatial\nresolution and large domain size to model as many features as possible, on\nvarious scales, within a particular system. In addition, the time-scales\ninvolved are usually wide-ranging also requiring significant amounts of CPU\ntime.\n  These two limits (resolution and time-scales) enforce huge limits on\ncomputational capabilities. The model we have developed therefore uses parallel\nalgorithms to increase the performance of standard serial methods. The aim of\nthis paper is to report the numerical methods we use and the techniques invoked\nfor parallelising the code. The justification of these methods is given by the\nextensive tests presented herein.", "abstract_full": "In this paper we describe a numerical method designed for modelling different\nkinds of astrophysical flows in three dimensions. Our method is a standard\nexplicit finite difference method employing the local shearing-box technique.\n  To model the features of astrophysical systems, which are usually\ncompressible, magnetised and turbulent, it is desirable to have high spatial\nresolution and large domain size to model as many features as possible, on\nvarious scales, within a particular system. In addition, the time-scales\ninvolved are usually wide-ranging also requiring significant amounts of CPU\ntime.\n  These two limits (resolution and time-scales) enforce huge limits on\ncomputational capabilities. The model we have developed therefore uses parallel\nalgorithms to increase the performance of standard serial methods. The aim of\nthis paper is to report the numerical methods we use and the techniques invoked\nfor parallelising the code. The justification of these methods is given by the\nextensive tests presented herein.", "authors": "S. E. Caunt, M. J. Korpi", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 6, "score": 0.8525660634040833, "paper_id": "astro-ph/0103001", "title": "Binaries and Globular Cluster Dynamics", "abstract": "We summarize the results of recent theoretical work on the dynamical\nevolution of globular clusters containing primordial binaries. Even a very\nsmall initial binary fraction (e.g., 10%) can play a key role in supporting a\ncluster against gravothermal collapse for many relaxation times. Inelastic\nencounters between binaries and single stars or other binaries provide a very\nsignificant energy source for the cluster. These dynamical interactions also\nlead to the production of large numbers of exotic systems such as ultracompact\nX-ray binaries, recycled radio pulsars, double degenerate systems, and blue\nstragglers. Our work is based on a new parallel supercomputer code implementing\nHenon's Monte Carlo method for simulating the dynamical evolution of dense\nstellar systems in the Fokker-Planck approximation. This new code allows us to\ncalculate very accurately the evolution of a cluster containing a realistic\nnumber of stars (N ~ 10^5 - 10^6) in typically a few hours to a few days of\ncomputing time. The discrete, star-by-star representation of the cluster in the\nsimulation makes it possible to treat naturally a number of important\nprocesses, including single and binary star evolution, all dynamical\ninteractions of single stars and binaries, and tidal interactions with the\nGalaxy.", "abstract_full": "We summarize the results of recent theoretical work on the dynamical\nevolution of globular clusters containing primordial binaries. Even a very\nsmall initial binary fraction (e.g., 10%) can play a key role in supporting a\ncluster against gravothermal collapse for many relaxation times. Inelastic\nencounters between binaries and single stars or other binaries provide a very\nsignificant energy source for the cluster. These dynamical interactions also\nlead to the production of large numbers of exotic systems such as ultracompact\nX-ray binaries, recycled radio pulsars, double degenerate systems, and blue\nstragglers. Our work is based on a new parallel supercomputer code implementing\nHenon's Monte Carlo method for simulating the dynamical evolution of dense\nstellar systems in the Fokker-Planck approximation. This new code allows us to\ncalculate very accurately the evolution of a cluster containing a realistic\nnumber of stars (N ~ 10^5 - 10^6) in typically a few hours to a few days of\ncomputing time. The discrete, star-by-star representation of the cluster in the\nsimulation makes it possible to treat naturally a number of important\nprocesses, including single and binary star evolution, all dynamical\ninteractions of single stars and binaries, and tidal interactions with the\nGalaxy.", "authors": "Frederic A. Rasio, John M. Fregeau, Kriten J. Joshi", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 7, "score": 0.85048907995224, "paper_id": "cond-mat/0103263", "title": "Implementation of parallel algorithms for 2D vortex dynamics simulation\n  in type-II superconductors", "abstract": "This report discusses the implementation of two parallel algorithms on a\ndistributed memory system for studying vortex dynamics in type-II\nsuperconductors. These algorithms are the same as that implemented for\nclassical molecular dynamics simulation with short-range forces (Plimpton, J.\nComp. Phys. Vol.117, 1 (1995)). The run time for parallel algorithm is tested\non a system containing upto 4 processors and compared with that for vectorized\nalgorithm on a single processor for system size ranging from 120 to 4800\nvortices.", "abstract_full": "This report discusses the implementation of two parallel algorithms on a\ndistributed memory system for studying vortex dynamics in type-II\nsuperconductors. These algorithms are the same as that implemented for\nclassical molecular dynamics simulation with short-range forces (Plimpton, J.\nComp. Phys. Vol.117, 1 (1995)). The run time for parallel algorithm is tested\non a system containing upto 4 processors and compared with that for vectorized\nalgorithm on a single processor for system size ranging from 120 to 4800\nvortices.", "authors": "Mahesh Chandran", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 8, "score": 0.8447602391242981, "paper_id": "cs/0103001", "title": "Construction of an algorithm in parallel for the Fast Fourier Transform", "abstract": "It has been designed,built and executed a code for the Fast Fourier Transform\n(FFT),compiled and executed in a cluster of 2^n computers under the operating\nsystem MacOS and using the routines MacMPI. As practical application,the code\nhas been used to obtain the transformed from an astronomic imagen,to execute a\nfilter on its and with a transformed inverse to recover the image with the\nvariates given by the filter.The computers arrangement are installed in the\nObservatorio Astronomico National in Colombia under the name OAN Cluster and in\nthis has been executed several applications.", "abstract_full": "It has been designed,built and executed a code for the Fast Fourier Transform\n(FFT),compiled and executed in a cluster of 2^n computers under the operating\nsystem MacOS and using the routines MacMPI. As practical application,the code\nhas been used to obtain the transformed from an astronomic imagen,to execute a\nfilter on its and with a transformed inverse to recover the image with the\nvariates given by the filter.The computers arrangement are installed in the\nObservatorio Astronomico National in Colombia under the name OAN Cluster and in\nthis has been executed several applications.", "authors": "G. Mario A. Higuera, Humberto Sarria, Diana Fonseca, John Idarraga", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
{"rank": 9, "score": 0.8433696031570435, "paper_id": "astro-ph/0103301", "title": "Multiscale Gaussian Random Fields for Cosmological Simulations", "abstract": "This paper describes the generation of initial conditions for numerical\nsimulations in cosmology with multiple levels of resolution, or multiscale\nsimulations. We present the theory of adaptive mesh refinement of Gaussian\nrandom fields followed by the implementation and testing of a computer code\npackage performing this refinement called GRAFIC2. This package is available to\nthe computational cosmology community at http://arcturus.mit.edu/grafic/ or by\nemail from the author.", "abstract_full": "This paper describes the generation of initial conditions for numerical\nsimulations in cosmology with multiple levels of resolution, or multiscale\nsimulations. We present the theory of adaptive mesh refinement of Gaussian\nrandom fields followed by the implementation and testing of a computer code\npackage performing this refinement called GRAFIC2. This package is available to\nthe computational cosmology community at http://arcturus.mit.edu/grafic/ or by\nemail from the author.", "authors": "Edmund Bertschinger", "year": "2001", "retrieved_at": "2025-10-26T17:01:12"}
