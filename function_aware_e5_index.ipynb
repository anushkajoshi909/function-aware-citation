{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff133939",
   "metadata": {},
   "source": [
    "\n",
    "# Function-Aware Retrieval: Build E5 Index from JSONL (unarXive subset)\n",
    "\n",
    "This notebook:\n",
    "1. Loads **4 JSONL** files (one JSON object per line; each line = one paper).\n",
    "2. Extracts `paper_id`, `title`, and `abstract` from each object.\n",
    "3. Builds **E5 embeddings** for `\"passage: <title> â€” <abstract>\"`.\n",
    "4. Saves a **FAISS** index (`index.faiss`) and aligned `meta.parquet`.\n",
    "5. Provides a **post-filter retrieval** function that:\n",
    "   - Retrieves **top-20** with E5,\n",
    "   - Runs your **function classifier** on those snippets,\n",
    "   - Filters/reranks by function probability.\n",
    "   \n",
    "> Plug in your own function classifier where indicated (we include a stub).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install packages (uncomment if your environment is missing any of these)\n",
    "# %pip install faiss-cpu pandas pyarrow sentence-transformers tqdm rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a68aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498ddb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- EDIT THESE PATHS IF NEEDED ----\n",
    "jsonl_files = [\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0101_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0102_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0103_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0104_001.jsonl\",\n",
    "]\n",
    "\n",
    "# Directory where we'll save the index + metadata\n",
    "out_dir = Path(\"e5_index_subset_1\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model to use: 'intfloat/e5-base-v2' is a good speed/quality trade-off\n",
    "E5_MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "\n",
    "# Limit (optional): set to None to index everything\n",
    "MAX_PAPERS = None  # e.g., 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a3bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in 11.76 s\n",
      "Warmup ok\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”’ Force CPU + tame threads + use local cache to avoid any network / CUDA shenanigans\n",
    "import os, torch, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"          # force no-GPU path\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # quieter + safer in notebooks\n",
    "os.environ[\"HF_HOME\"] = \"./hf_cache\"             # local cache (no network)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "E5_MODEL_NAME = \"intfloat/e5-small-v2\"  # swap to e5-base-v2 later\n",
    "\n",
    "t0 = time.time()\n",
    "model = SentenceTransformer(E5_MODEL_NAME, device=\"cpu\", cache_folder=\"./hf_cache\")\n",
    "print(\"Loaded model in\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# warmup to avoid first-call lag\n",
    "_ = model.encode([\"query: warmup\"], normalize_embeddings=True)\n",
    "print(\"Warmup ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f846b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading arXiv_src_0101_001.jsonl: 2319it [00:02, 896.69it/s] \n",
      "Reading arXiv_src_0102_001.jsonl: 2192it [00:02, 962.40it/s] \n",
      "Reading arXiv_src_0103_001.jsonl: 2441it [00:02, 1001.08it/s]\n",
      "Reading arXiv_src_0104_001.jsonl: 2310it [00:02, 854.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9262 unique papers\n",
      "           paper_id                                              title  \\\n",
      "0  quant-ph/0101147               Radiation trapping in coherent media   \n",
      "1  quant-ph/0101145  Mimicking a Kerrlike medium in the dispersive ...   \n",
      "2  quant-ph/0101144  What is Possible Without Disturbing Partially ...   \n",
      "\n",
      "                                             authors  year  \n",
      "0  [A. B. Matsko, I. Novikova, M. O. Scully, G. R...  2001  \n",
      "1     [A. B. Klimov, L. L. Sanchez-Soto, J. Delgado]  2001  \n",
      "2                    [Masato Koashi, Nobuyuki Imoto]  2001  \n"
     ]
    }
   ],
   "source": [
    "# build_meta_with_authors.py (cell)\n",
    "import json, ast, re, os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "MAX_PAPERS = None  # or an int to truncate during dev\n",
    "\n",
    "_yr_re = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def best_year_from_obj(obj):\n",
    "    for key in (\"year\",\"published\",\"date\",\"update_date\",\"created\"):\n",
    "        if key in obj and obj[key]:\n",
    "            s = str(obj[key])\n",
    "            try:\n",
    "                y = int(s[:4])\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "            except Exception:\n",
    "                m = _yr_re.search(s)\n",
    "                if m: return int(m.group(0))\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    pid = obj.get(\"paper_id\") or md.get(\"id\") or obj.get(\"id\") or md.get(\"arxiv_id\") or obj.get(\"arxiv_id\")\n",
    "    if isinstance(pid, str) and \"/\" in pid:\n",
    "        try:\n",
    "            yy = int(pid.split(\"/\")[1][:2])\n",
    "            return 2000 + yy if yy < 50 else 1900 + yy\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def extract_title_abstract(obj):\n",
    "    title = None\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    title = md.get(\"title\") or obj.get(\"title\")\n",
    "    abstract = None\n",
    "    if isinstance(md.get(\"abstract\"), str):\n",
    "        abstract = md[\"abstract\"]\n",
    "    if not abstract and isinstance(obj.get(\"abstract\"), dict):\n",
    "        abstract = obj[\"abstract\"].get(\"text\")\n",
    "    if not abstract:\n",
    "        abstract = obj.get(\"abstract\")\n",
    "    return title, abstract\n",
    "\n",
    "def norm_raw_authors(raw):\n",
    "    if raw is None: return []\n",
    "    if isinstance(raw, (list, tuple)): return [str(x).strip() for x in raw if str(x).strip()]\n",
    "    s = str(raw).strip()\n",
    "    if not s: return []\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try: data = json.loads(s)\n",
    "        except Exception:\n",
    "            try: data = ast.literal_eval(s)\n",
    "            except Exception: data = None\n",
    "        if isinstance(data, list): return norm_raw_authors(data)\n",
    "    sep = \";\" if \";\" in s else \",\"\n",
    "    return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "\n",
    "def authors_from_parsed(ap):\n",
    "    out=[]\n",
    "    if isinstance(ap, list):\n",
    "        for it in ap:\n",
    "            if isinstance(it, dict):\n",
    "                nm=(\" \".join([it.get(\"first\",\"\"), it.get(\"last\",\"\")])).strip()\n",
    "            elif isinstance(it, (list,tuple)):\n",
    "                last=str(it[0]).strip() if len(it)>0 else \"\"\n",
    "                first=str(it[1]).strip() if len(it)>1 else \"\"\n",
    "                nm=(\" \".join([first,last])).strip()\n",
    "            else:\n",
    "                nm=str(it).strip()\n",
    "            if nm: out.append(nm)\n",
    "    return out\n",
    "\n",
    "def authors_from_obj(obj):\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    if \"authors_parsed\" in obj:\n",
    "        a = authors_from_parsed(obj[\"authors_parsed\"])\n",
    "        if a: return a\n",
    "    if \"authors_parsed\" in md:\n",
    "        a = authors_from_parsed(md[\"authors_parsed\"])\n",
    "        if a: return a\n",
    "    if \"authors\" in obj:\n",
    "        a = norm_raw_authors(obj[\"authors\"])\n",
    "        if a: return a\n",
    "    if \"authors\" in md:\n",
    "        a = norm_raw_authors(md[\"authors\"])\n",
    "        if a: return a\n",
    "    return []\n",
    "\n",
    "def get_pid(obj):\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    return obj.get(\"paper_id\") or md.get(\"id\") or obj.get(\"id\") or md.get(\"arxiv_id\") or obj.get(\"arxiv_id\")\n",
    "\n",
    "rows = []\n",
    "for path in jsonl_files:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Reading {os.path.basename(path)}\"):\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            pid = get_pid(obj)\n",
    "            title, abstract = extract_title_abstract(obj)\n",
    "            if not pid or not title or not abstract:\n",
    "                continue\n",
    "            authors = authors_from_obj(obj)\n",
    "            year = best_year_from_obj(obj)\n",
    "            rows.append({\n",
    "                \"paper_id\": pid,\n",
    "                \"title\": title.strip(),\n",
    "                \"abstract\": str(abstract).strip(),\n",
    "                \"authors\": authors,\n",
    "                \"year\": year\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df.drop_duplicates(subset=[\"paper_id\"]).reset_index(drop=True)\n",
    "if MAX_PAPERS: df = df.head(MAX_PAPERS)\n",
    "print(f\"Loaded {len(df)} unique papers\")\n",
    "print(df.head(3)[[\"paper_id\",\"title\",\"authors\",\"year\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119c79e",
   "metadata": {},
   "source": [
    "## 1) Load and parse JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f98c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9262 unique papers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quant-ph/0101147</td>\n",
       "      <td>Radiation trapping in coherent media</td>\n",
       "      <td>We show that the effective decay rate of Zeema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quant-ph/0101145</td>\n",
       "      <td>Mimicking a Kerrlike medium in the dispersive ...</td>\n",
       "      <td>We find an effective Hamiltonian describing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quant-ph/0101144</td>\n",
       "      <td>What is Possible Without Disturbing Partially ...</td>\n",
       "      <td>Consider a situation in which a quantum system...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           paper_id                                              title  \\\n",
       "0  quant-ph/0101147               Radiation trapping in coherent media   \n",
       "1  quant-ph/0101145  Mimicking a Kerrlike medium in the dispersive ...   \n",
       "2  quant-ph/0101144  What is Possible Without Disturbing Partially ...   \n",
       "\n",
       "                                            abstract  \n",
       "0  We show that the effective decay rate of Zeema...  \n",
       "1  We find an effective Hamiltonian describing th...  \n",
       "2  Consider a situation in which a quantum system...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skip\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def extract_title_abstract(obj):\n",
    "    # Title\n",
    "    title = None\n",
    "    # Try standard location\n",
    "    if isinstance(obj.get(\"metadata\"), dict):\n",
    "        title = obj[\"metadata\"].get(\"title\")\n",
    "    if not title:\n",
    "        # fallback: sometimes title may be at top-level (rare)\n",
    "        title = obj.get(\"title\")\n",
    "    # Abstract\n",
    "    abstract = None\n",
    "    # Try metadata.abstract (string) first\n",
    "    if isinstance(obj.get(\"metadata\"), dict) and isinstance(obj[\"metadata\"].get(\"abstract\"), str):\n",
    "        abstract = obj[\"metadata\"][\"abstract\"]\n",
    "    # Fallback to top-level 'abstract' object: {\"section\": \"...\", \"text\": \"...\"}\n",
    "    if not abstract and isinstance(obj.get(\"abstract\"), dict):\n",
    "        abstract = obj[\"abstract\"].get(\"text\")\n",
    "    # final fallback\n",
    "    if not abstract and isinstance(obj.get(\"abstract\"), str):\n",
    "        abstract = obj[\"abstract\"]\n",
    "    return title, abstract\n",
    "\n",
    "rows = []\n",
    "for p in jsonl_files:\n",
    "    p = Path(p)\n",
    "    if not p.exists():\n",
    "        print(f\"WARNING: missing file -> {p}\")\n",
    "        continue\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception as e:\n",
    "                # sometimes lines can be malformed; skip\n",
    "                continue\n",
    "            paper_id = obj.get(\"paper_id\") or obj.get(\"id\")\n",
    "            title, abstract = extract_title_abstract(obj)\n",
    "            if not paper_id or not title or not abstract:\n",
    "                continue\n",
    "            rows.append({\"paper_id\": paper_id, \"title\": title.strip(), \"abstract\": abstract.strip()})\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"paper_id\"]).reset_index(drop=True)\n",
    "if MAX_PAPERS is not None:\n",
    "    df = df.head(MAX_PAPERS)\n",
    "\n",
    "print(f\"Loaded {len(df)} unique papers\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebdd435",
   "metadata": {},
   "source": [
    "## 2) Encode with E5 and build FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3504b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2de71dae5fb4a818c56ec3f7a3242eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: e5_index_subset_1/index.faiss and e5_index_subset_1/meta.parquet\n"
     ]
    }
   ],
   "source": [
    "#Skip running this block\n",
    "# Prepare texts for embeddings\n",
    "texts = [f\"passage: {t} â€” {a}\" for t, a in zip(df['title'].tolist(), df['abstract'].tolist())]\n",
    "\n",
    "# Encode (L2-normalized embeddings give cosine via inner product)\n",
    "emb = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True).astype(\"float32\")\n",
    "dim = emb.shape[1]\n",
    "\n",
    "# Build FAISS index (cosine via inner product on normalized vectors)\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(emb)\n",
    "\n",
    "# Save index + metadata\n",
    "faiss.write_index(index, str(out_dir / \"index.faiss\"))\n",
    "df.to_parquet(out_dir / \"meta.parquet\", index=False)\n",
    "\n",
    "print(f\"Saved: {out_dir/'index.faiss'} and {out_dir/'meta.parquet'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b967b25",
   "metadata": {},
   "source": [
    "## 3) Query + post-filter (top-20 â†’ function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716962ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Reload index & metadata\n",
    "index = faiss.read_index(str(out_dir / \"index.faiss\"))\n",
    "meta  = pd.read_parquet(out_dir / \"meta.parquet\")\n",
    "\n",
    "# âœ… Reuse the already-loaded model from earlier\n",
    "q_model = model                      # <-- do NOT call SentenceTransformer() again\n",
    "_ = q_model.encode([\"query: warmup\"], normalize_embeddings=True)  # quick warmup\n",
    "\n",
    "def encode_query(q: str):\n",
    "    return q_model.encode([f\"query: {q}\"], normalize_embeddings=True).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3c132",
   "metadata": {},
   "source": [
    "### 3.a) Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "581cd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# helpers (format/display)\n",
    "# -----------------------\n",
    "\n",
    "def _trim(text, max_chars=450):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = str(text).strip()\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    cut = s[:max_chars].rsplit(\" \", 1)[0]\n",
    "    return cut + \"â€¦\"\n",
    "\n",
    "\n",
    "def _format_authors(a, k=3):\n",
    "    if a is None:\n",
    "        return []\n",
    "    if isinstance(a, (list, tuple)):\n",
    "        names = [str(x).strip() for x in a if str(x).strip()]\n",
    "    else:\n",
    "        sep = \";\" if \";\" in str(a) else \",\"\n",
    "        names = [t.strip() for t in str(a).split(sep) if t.strip()]\n",
    "    if not names:\n",
    "        return []\n",
    "    return names[:k] + ([\"et al.\"] if len(names) > k else [])\n",
    "\n",
    "\n",
    "_yr_re = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def _best_year(row):\n",
    "    for key in (\"year\", \"published\", \"date\", \"update_date\", \"created\"):\n",
    "        if key in row and pd.notna(row[key]):\n",
    "            try:\n",
    "                y = int(str(row[key])[:4])\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "            except Exception:\n",
    "                pass\n",
    "            m = _yr_re.search(str(row[key]))\n",
    "            if m:\n",
    "                return int(m.group(0))\n",
    "    pid = row.get(\"paper_id\") or row.get(\"arxiv_id\") or row.get(\"id\")\n",
    "    if isinstance(pid, str) and \"/\" in pid:\n",
    "        try:\n",
    "            yy = int(pid.split(\"/\")[1][:2])\n",
    "            return 2000 + yy if yy < 50 else 1900 + yy\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _extract_abstract(abstract_field):\n",
    "    if abstract_field is None:\n",
    "        return \"\"\n",
    "    if isinstance(abstract_field, dict):\n",
    "        return str(abstract_field.get(\"text\") or abstract_field.get(\"abstract\") or \"\")\n",
    "    return str(abstract_field)\n",
    "\n",
    "# -----------------------\n",
    "# stopwords + token utils\n",
    "# -----------------------\n",
    "\n",
    "DEFAULT_STOPWORDS = {\n",
    "    \"a\",\"an\",\"and\",\"the\",\"of\",\"to\",\"in\",\"on\",\"for\",\"with\",\"by\",\"as\",\"at\",\"or\",\"but\",\"if\",\"than\",\"then\",\n",
    "    \"from\",\"into\",\"over\",\"under\",\"between\",\"within\",\"without\",\"about\",\"via\",\"per\",\"through\",\"across\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"do\",\"does\",\"did\",\"can\",\"could\",\n",
    "    \"may\",\"might\",\"will\",\"would\",\"shall\",\"should\",\"must\",\"not\",\"no\",\"nor\",\"also\",\"both\",\"either\",\"neither\",\n",
    "    \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"their\",\"our\",\"your\",\"his\",\"her\",\"them\",\"they\",\"we\",\"you\",\"i\",\n",
    "    \"such\",\"thus\",\"there\",\"here\",\"where\",\"when\",\"which\",\"who\",\"whom\",\"whose\",\"what\",\"why\",\"how\",\n",
    "    \"using\",\"use\",\"used\",\"based\",\"approach\",\"approaches\",\"method\",\"methods\",\"result\",\"results\",\"show\",\n",
    "    \"shows\",\"shown\",\"paper\",\"study\",\"work\",\"new\"\n",
    "}\n",
    "\n",
    "\n",
    "def minmax_norm(x):\n",
    "    x = np.asarray(x, dtype=np.float32).reshape(-1)\n",
    "    if x.size == 0:\n",
    "        return x\n",
    "    lo, hi = float(np.min(x)), float(np.max(x))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or (hi - lo) < 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - lo) / (hi - lo)\n",
    "\n",
    "\n",
    "_token_re = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return [t.lower() for t in _token_re.findall(str(text))]\n",
    "\n",
    "\n",
    "def content_terms(tokens, stopwords, min_len=3):\n",
    "    return [t for t in tokens if not t.isdigit() and len(t) >= min_len and t not in stopwords]\n",
    "\n",
    "# -----------------------\n",
    "# robust author extraction\n",
    "# -----------------------\n",
    "\n",
    "def _authors_from_row(row):\n",
    "\n",
    "    def _norm_raw_authors(raw):\n",
    "        if raw is None:\n",
    "            return []\n",
    "        if isinstance(raw, (list, tuple)):\n",
    "            return [str(x).strip() for x in raw if str(x).strip()]\n",
    "        s = str(raw).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                data = json.loads(s)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "            if isinstance(data, list):\n",
    "                return _norm_raw_authors(data)\n",
    "        sep = \";\" if \";\" in s else \",\"\n",
    "        return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "\n",
    "    def _norm_authors_parsed(ap):\n",
    "        out = []\n",
    "        if isinstance(ap, (list, tuple)):\n",
    "            for item in ap:\n",
    "                if isinstance(item, (list, tuple)):\n",
    "                    last = str(item[0]).strip() if len(item) > 0 else \"\"\n",
    "                    first = str(item[1]).strip() if len(item) > 1 else \"\"\n",
    "                    name = \" \".join([first, last]).strip()\n",
    "                    if name:\n",
    "                        out.append(name)\n",
    "                elif isinstance(item, dict):\n",
    "                    first = str(item.get(\"first\", \"\")).strip()\n",
    "                    last = str(item.get(\"last\", \"\")).strip()\n",
    "                    name = \" \".join([first, last]).strip()\n",
    "                    if name:\n",
    "                        out.append(name)\n",
    "                else:\n",
    "                    s = str(item).strip()\n",
    "                    if s:\n",
    "                        out.append(s)\n",
    "        elif isinstance(ap, str) and ap.strip():\n",
    "            try:\n",
    "                data = json.loads(ap)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(ap)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "            if isinstance(data, list):\n",
    "                return _norm_authors_parsed(data)\n",
    "        return out\n",
    "\n",
    "    # direct columns\n",
    "    if \"authors\" in row and pd.notna(row[\"authors\"]):\n",
    "        names = _norm_raw_authors(row[\"authors\"])\n",
    "        if names:\n",
    "            return names\n",
    "\n",
    "    if \"authors_parsed\" in row and pd.notna(row[\"authors_parsed\"]):\n",
    "        names = _norm_authors_parsed(row[\"authors_parsed\"])\n",
    "        if names:\n",
    "            return names\n",
    "\n",
    "    # nested metadata dict (JSONL)\n",
    "    if \"metadata\" in row and isinstance(row[\"metadata\"], dict):\n",
    "        if \"authors\" in row[\"metadata\"]:\n",
    "            names = _norm_raw_authors(row[\"metadata\"][\"authors\"])\n",
    "            if names:\n",
    "                return names\n",
    "        if \"authors_parsed\" in row[\"metadata\"]:\n",
    "            names = _norm_authors_parsed(row[\"metadata\"][\"authors_parsed\"])\n",
    "            if names:\n",
    "                return names\n",
    "\n",
    "    return []\n",
    "\n",
    "# -----------------------\n",
    "# meta building from JSONL\n",
    "# -----------------------\n",
    "\n",
    "def _rows_from_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            md = obj.get(\"metadata\", {}) or {}\n",
    "            authors_field = md.get(\"authors\") or obj.get(\"authors\")\n",
    "            # could be list, string, or None â€” leave as is for _authors_from_row to normalize\n",
    "            yield {\n",
    "                \"paper_id\": obj.get(\"paper_id\") or md.get(\"id\"),\n",
    "                \"title\": md.get(\"title\", \"\") or obj.get(\"title\", \"\"),\n",
    "                \"abstract\": obj.get(\"abstract\", md.get(\"abstract\", \"\")),\n",
    "                \"authors\": authors_field,   # <- flat authors column\n",
    "                \"metadata\": md,             # <- keep raw metadata for fallback\n",
    "            }\n",
    "\n",
    "\n",
    "def build_meta_from_jsonl(paths):\n",
    "    \"\"\"\n",
    "    paths: str or List[str] of jsonl files\n",
    "    Returns a pandas.DataFrame with columns:\n",
    "    paper_id | title | abstract | metadata\n",
    "    \"\"\"\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "    all_rows = []\n",
    "    for p in paths:\n",
    "        for row in _rows_from_jsonl(p):\n",
    "            all_rows.append(row)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Basic cleanup / de-dup\n",
    "    # Prefer keeping the first occurrence per paper_id, then title\n",
    "    if \"paper_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"paper_id\"], keep=\"first\")\n",
    "    if \"title\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# main search function\n",
    "# -----------------------\n",
    "\n",
    "# COSINE-ONLY retrieval (overwrites any previous definition)\n",
    "def search_post_filter(\n",
    "    query,\n",
    "    desired_function,          # kept only for provenance; NOT used for ranking\n",
    "    topN=20,\n",
    "    topK_return=10,\n",
    "    normalize_scores=False,\n",
    "    stopwords=None,\n",
    "    min_term_len=3,\n",
    "    abstract_chars=450,\n",
    "    authors_shown=3\n",
    "):\n",
    "    stopwords = DEFAULT_STOPWORDS if stopwords is None else set(stopwords)\n",
    "\n",
    "    # 1) semantic retrieve\n",
    "    qv = encode_query(query)\n",
    "    scores, idxs = index.search(qv, topN)\n",
    "    scores, idxs = scores[0].astype(np.float32), idxs[0]\n",
    "\n",
    "    # 2) order purely by cosine\n",
    "    order = np.argsort(-scores)\n",
    "    display_scores = minmax_norm(scores) if normalize_scores else scores\n",
    "\n",
    "    # 3) lexical explainers\n",
    "    q_terms_all  = tokenize(query)\n",
    "    q_terms_used = content_terms(q_terms_all, stopwords, min_len=min_term_len)\n",
    "\n",
    "    out = []\n",
    "    for rank_pos in range(min(topK_return, len(order))):\n",
    "        r = order[rank_pos]\n",
    "        row = meta.iloc[idxs[r]]\n",
    "\n",
    "        title = row.get(\"title\", \"\")\n",
    "        abstract_txt = _extract_abstract(row.get(\"abstract\", \"\"))\n",
    "        paper_id = row.get(\"paper_id\") or row.get(\"arxiv_id\") or row.get(\"id\")\n",
    "\n",
    "        title_tokens = content_terms(tokenize(title), stopwords, min_len=min_term_len)\n",
    "        abs_tokens   = content_terms(tokenize(abstract_txt), stopwords, min_len=min_term_len)\n",
    "\n",
    "        title_matches = sorted(set(q_terms_used) & set(title_tokens))\n",
    "        abs_matches   = sorted(set(q_terms_used) & set(abs_tokens))\n",
    "\n",
    "        authors_list = _authors_from_row(row)\n",
    "        authors_fmt  = _format_authors(authors_list, k=authors_shown)\n",
    "\n",
    "        out.append({\n",
    "            \"score\": float(display_scores[r]),\n",
    "            \"cosine\": float(scores[r]),\n",
    "            \"title\": title,\n",
    "            \"abstract\": _trim(abstract_txt, abstract_chars),\n",
    "            \"arxiv_id\": paper_id,\n",
    "            \"year\": _best_year(row),\n",
    "            \"authors\": authors_fmt,\n",
    "            \"title_matches\": title_matches,\n",
    "            \"abs_matches\": abs_matches,\n",
    "            \"query_terms_used\": q_terms_used,\n",
    "            \"function_requested\": str(desired_function) if desired_function is not None else \"\",\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# convenience: results -> DataFrame\n",
    "# -----------------------\n",
    "\n",
    "def to_df(res):\n",
    "    cols = [\"cosine\",\"p_function\",\"title\",\"year\",\"arxiv_id\",\"authors\",\"title_matches\",\"abs_matches\"]\n",
    "    return pd.DataFrame([{k: r.get(k) for k in cols} for r in res])\n",
    "\n",
    "\n",
    "#meta = build_meta_from_jsonl(jsonl_files)\n",
    "#print(len(meta), \"papers loaded\")\n",
    "#print(meta.head(5)[[\"paper_id\",\"title\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e3912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def _safe_join(x):\n",
    "    if x is None: return \"\"\n",
    "    if isinstance(x, list): return \", \".join(str(t) for t in x)\n",
    "    return str(x)\n",
    "\n",
    "def display_results_table(results):\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            \"Title\": r.get(\"title\", \"\"),\n",
    "            \"Year\": r.get(\"year\", \"\"),\n",
    "            \"Authors\": _safe_join(r.get(\"authors\", [])),\n",
    "            \"Abstract\": r.get(\"abstract\", \"\"),\n",
    "            \"Title Matches\": _safe_join(r.get(\"title_matches\", [])),\n",
    "            \"Abstract Matches\": _safe_join(r.get(\"abs_matches\", [])),\n",
    "            \"Score\": r.get(\"cosine_norm\", r.get(\"cosine\", None)),\n",
    "        } for r in (results or [])\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe2c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_current_classification(\n",
    "    classified_path=\"classified_outputs.jsonl\",\n",
    "    out_dir=\"outputs\",\n",
    "    topN=50,\n",
    "    topK_return=10,\n",
    "    normalize_scores=True,\n",
    "    debug=False\n",
    "):\n",
    "    import os, json, pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    def read_last_jsonl(path: str):\n",
    "        \"\"\"Return the last non-empty JSON object from a .jsonl file.\"\"\"\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        if not lines:\n",
    "            raise ValueError(f\"No lines found in {path}\")\n",
    "        try:\n",
    "            return json.loads(lines[-1])\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Last line is not valid JSON: {e}\\nLine: {lines[-1][:200]}...\")\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_jsonl = os.path.join(out_dir, \"topk_candidates.jsonl\")\n",
    "    out_csv   = os.path.join(out_dir, \"topk_candidates.csv\")\n",
    "\n",
    "    # --- Read the most recent block (last line), not the first line ---\n",
    "    obj = read_last_jsonl(classified_path)\n",
    "\n",
    "    query = obj.get(\"query\", \"\")\n",
    "    sent_list = obj.get(\"sentence_classification\", []) or []\n",
    "    if debug:\n",
    "        print(f\"[debug] loaded {len(sent_list)} sentences from latest block\")\n",
    "\n",
    "    rows = []\n",
    "    for si, s in enumerate(sent_list):\n",
    "        text = (s.get(\"sentence\") or \"\").strip()\n",
    "        needs_cit = bool(s.get(\"needs_citation\", True))\n",
    "        just = s.get(\"justification\", \"\")\n",
    "\n",
    "        # --- Correctly read plural key and normalize ---\n",
    "        func_list = s.get(\"citation_functions\")\n",
    "        if func_list is None:\n",
    "            # graceful fallback if upstream ever writes singular by mistake\n",
    "            func_list = s.get(\"citation_function\")\n",
    "        if isinstance(func_list, str):\n",
    "            func_list = [func_list]\n",
    "        if not isinstance(func_list, list):\n",
    "            func_list = []\n",
    "\n",
    "        func_first = (func_list[0] if func_list else \"\").strip().lower()\n",
    "        desired_function = func_first or \"background\"  # fallback so retrieval still works\n",
    "\n",
    "        retrieved_at = datetime.utcnow().isoformat()\n",
    "\n",
    "        try:\n",
    "            results = search_post_filter(\n",
    "                query=text if text else query,     # prefer sentence; fallback to full query\n",
    "                desired_function=desired_function, # single normalized label\n",
    "                topN=topN,\n",
    "                topK_return=topK_return,\n",
    "                normalize_scores=normalize_scores\n",
    "            ) or []\n",
    "        except Exception as e:\n",
    "            if debug: print(f\"[si{si}] retrieval error: {e}\")\n",
    "            results = []\n",
    "\n",
    "        if results:\n",
    "            for rank, r in enumerate(results):\n",
    "                rows.append({\n",
    "                    \"sentence_idx\": si,\n",
    "                    \"sentence_uid\": f\"blk1|si{si}\",\n",
    "                    \"sentence_text\": text,\n",
    "                    \"citation_function\": desired_function,   # <- always a string\n",
    "                    \"citation_functions\": func_list,         # keep original list for traceability\n",
    "                    \"needs_citation\": needs_cit,\n",
    "                    \"justification\": just,\n",
    "                    \"rank\": rank,\n",
    "                    \"paper_id\": r.get(\"arxiv_id\") or r.get(\"paper_id\") or \"\",\n",
    "                    \"title\": r.get(\"title\", \"\"),\n",
    "                    \"year\": r.get(\"year\", \"\"),\n",
    "                    \"authors\": \", \".join(r.get(\"authors\", [])) if isinstance(r.get(\"authors\"), list)\n",
    "                               else (r.get(\"authors\") or \"\"),\n",
    "                    \"abstract\": r.get(\"abstract\", \"\"),\n",
    "                    \"normalized_score\": r.get(\"cosine_norm\", r.get(\"cosine\")),\n",
    "                    \"title_matches\": \", \".join(r.get(\"title_matches\", [])) if isinstance(r.get(\"title_matches\"), list)\n",
    "                                     else (r.get(\"title_matches\") or \"\"),\n",
    "                    \"abs_matches\": \", \".join(r.get(\"abs_matches\", [])) if isinstance(r.get(\"abs_matches\"), list)\n",
    "                                   else (r.get(\"abs_matches\") or \"\"),\n",
    "                    \"retrieval_error\": None,\n",
    "                    \"retrieved_at\": retrieved_at\n",
    "                })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"sentence_idx\": si,\n",
    "                \"sentence_uid\": f\"blk1|si{si}\",\n",
    "                \"sentence_text\": text,\n",
    "                \"citation_function\": desired_function,\n",
    "                \"citation_functions\": func_list,\n",
    "                \"needs_citation\": needs_cit,\n",
    "                \"justification\": just,\n",
    "                \"rank\": None,\n",
    "                \"paper_id\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"year\": \"\",\n",
    "                \"authors\": \"\",\n",
    "                \"abstract\": \"\",\n",
    "                \"normalized_score\": None,\n",
    "                \"title_matches\": \"\",\n",
    "                \"abs_matches\": \"\",\n",
    "                \"retrieval_error\": None,\n",
    "                \"retrieved_at\": retrieved_at\n",
    "            })\n",
    "\n",
    "    # Overwrite outputs each run\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in rows:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Sentences: {len(sent_list)} | rows written: {len(rows)}\")\n",
    "        print(f\"- {out_jsonl}\\n- {out_csv}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16abdd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentence_idx citation_function\n",
      "0              0        motivation\n",
      "10             1        futurework\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"outputs/topk_candidates.csv\")\n",
    "print(df[[\"sentence_idx\",\"citation_function\"]].drop_duplicates().sort_values([\"sentence_idx\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8e6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cosine function_requested  \\\n",
      "0  0.828061            COMPARE   \n",
      "1  0.827704            COMPARE   \n",
      "2  0.826666            COMPARE   \n",
      "3  0.826383            COMPARE   \n",
      "4  0.825162            COMPARE   \n",
      "5  0.825125            COMPARE   \n",
      "6  0.824200            COMPARE   \n",
      "7  0.823486            COMPARE   \n",
      "8  0.822597            COMPARE   \n",
      "9  0.822466            COMPARE   \n",
      "\n",
      "                                               title  year          arxiv_id  \\\n",
      "0      Microscopic Reaction Dynamics at SPS and RHIC  2001   nucl-th/0104040   \n",
      "1  Jet Quenching and the p-bar >= pi- Anomaly at ...  2001   nucl-th/0104066   \n",
      "2      New results on the temporal structure of GRBs  2001  astro-ph/0103011   \n",
      "3  On the mean field treatment of attractive inte...  2001  cond-mat/0104317   \n",
      "4  Differential Cross Sections Measurement for th...  2001   nucl-ex/0101001   \n",
      "5  A contiuum model for low temperature relaxatio...  2001  cond-mat/0104235   \n",
      "6  A semi-analytical approach to non-linear shock...  2001  astro-ph/0104064   \n",
      "7  Vibrational origin of the fast relaxation proc...  2001  cond-mat/0104265   \n",
      "8                                           1D Aging  2001  cond-mat/0103494   \n",
      "9  On Echo Outbursts and ER UMa Supercycles in SU...  2001  astro-ph/0101102   \n",
      "\n",
      "                                             authors title_matches  \\\n",
      "0                                  [Steffen A. Bass]            []   \n",
      "1                        [Ivan VitevMiklos Gyulassy]            []   \n",
      "2                                 [E. NakarT. Piran]            []   \n",
      "3                       [Kirill KatsovJohn D. Weeks]      [simple]   \n",
      "4  [M. BetigeriJ. BojowaldA. BudzanowskiA . Chatt...            []   \n",
      "5                                  [O. Pierre-Louis]            []   \n",
      "6                                   [Pasquale Blasi]            []   \n",
      "7                       [S. MossaG. MonacoG. Ruocco]            []   \n",
      "8      [L. R. FontesM. IsopiC. M. NewmanD. L. Stein]            []   \n",
      "9                                     [Coel Hellier]            []   \n",
      "\n",
      "   abs_matches  \n",
      "0     [models]  \n",
      "1  [reactions]  \n",
      "2           []  \n",
      "3     [simple]  \n",
      "4           []  \n",
      "5           []  \n",
      "6     [simple]  \n",
      "7           []  \n",
      "8     [quench]  \n",
      "9           []  \n"
     ]
    }
   ],
   "source": [
    "# Skip this block\n",
    "# # force authors into strings\n",
    "meta[\"authors\"] = meta[\"authors\"].apply(lambda x: x if isinstance(x, str) or x is None else str(x))\n",
    "\n",
    "# Example: one clause from a generated answer\n",
    "clause = \"Compared to simple quench models, back reactions temper and delay amplification.\"\n",
    "func   = \"COMPARE\"\n",
    "\n",
    "results = search_post_filter(\n",
    "    query=clause,\n",
    "    desired_function=func,\n",
    "    topN=50,\n",
    "    topK_return=10\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for inspection\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results[[\"cosine\",\"function_requested\",\"title\",\"year\",\"arxiv_id\",\"authors\",\"title_matches\",\"abs_matches\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a06895",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "- Replace `FunctionClassifierStub` with your **trained function classifier**.\n",
    "- If you need **BM25** fusion for acronyms/symbols, we can add a BM25 index and blend scores.\n",
    "- For full-text retrieval, split papers into **paragraphs/sentences** and index those instead of abstracts.\n",
    "- If your set grows beyond ~1â€“2M passages, consider FAISS **IVF-PQ** or **HNSW** for faster search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecfaaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] loaded 2 sentences from latest block\n",
      "Sentences: 2 | rows written: 20\n",
      "- outputs/topk_candidates.jsonl\n",
      "- outputs/topk_candidates.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>sentence_uid</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>citation_function</th>\n",
       "      <th>citation_functions</th>\n",
       "      <th>needs_citation</th>\n",
       "      <th>justification</th>\n",
       "      <th>rank</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>normalized_score</th>\n",
       "      <th>title_matches</th>\n",
       "      <th>abs_matches</th>\n",
       "      <th>retrieval_error</th>\n",
       "      <th>retrieved_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>0</td>\n",
       "      <td>math/0104121</td>\n",
       "      <td>Eigenvalue estimates of the Dirac operator dep...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Thomas FriedrichKlaus-Dieter Kirchberg</td>\n",
       "      <td>We prove a new lower bound for the first eigen...</td>\n",
       "      <td>0.861814</td>\n",
       "      <td>dirac, estimates</td>\n",
       "      <td>bounds, curvature, dirac, manifolds, riemannia...</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>1</td>\n",
       "      <td>math/0103095</td>\n",
       "      <td>On eigenvalue estimates for the Dirac operator</td>\n",
       "      <td>2001</td>\n",
       "      <td>N. GinouxB. Morel</td>\n",
       "      <td>We give lower bounds for the eigenvalues of th...</td>\n",
       "      <td>0.855258</td>\n",
       "      <td>dirac, estimates</td>\n",
       "      <td>bounds, curvature, dirac, operators</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>2</td>\n",
       "      <td>math/0101061</td>\n",
       "      <td>Spectral estimates on 2-tori</td>\n",
       "      <td>2001</td>\n",
       "      <td>Bernd Ammann</td>\n",
       "      <td>We prove upper and lower bounds for the eigenv...</td>\n",
       "      <td>0.851397</td>\n",
       "      <td>estimates</td>\n",
       "      <td>bounds, dirac, riemannian, spin, uses</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>3</td>\n",
       "      <td>math/0101111</td>\n",
       "      <td>Eigenvalue estimates for the Dirac-Schr\\\"oding...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Bertrand Morel</td>\n",
       "      <td>We give new estimates for the eigenvalues of t...</td>\n",
       "      <td>0.843709</td>\n",
       "      <td>dirac, estimates, operators</td>\n",
       "      <td>curvature, dirac, estimates, inequalities</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>4</td>\n",
       "      <td>math/0102135</td>\n",
       "      <td>New bounds on Kakeya problems</td>\n",
       "      <td>2001</td>\n",
       "      <td>Nets KatzTerence Tao</td>\n",
       "      <td>We establish new estimates on the Minkowski an...</td>\n",
       "      <td>0.830213</td>\n",
       "      <td>bounds</td>\n",
       "      <td>bounds, establish, estimates</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>5</td>\n",
       "      <td>quant-ph/0103089</td>\n",
       "      <td>Geometrisation of electromagnetic field and to...</td>\n",
       "      <td>2001</td>\n",
       "      <td>O. A. Olkhov</td>\n",
       "      <td>A new approach is proposed for an electromagne...</td>\n",
       "      <td>0.825784</td>\n",
       "      <td></td>\n",
       "      <td>curvature, dirac</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>6</td>\n",
       "      <td>math/0101084</td>\n",
       "      <td>Curvature Estimates in Asymptotically Flat Man...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Felix FinsterInes Kath</td>\n",
       "      <td>We consider an asymptotically flat Riemannian ...</td>\n",
       "      <td>0.824458</td>\n",
       "      <td>curvature, estimates, manifolds</td>\n",
       "      <td>bounds, curvature, riemannian, spin</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>7</td>\n",
       "      <td>hep-th/0103206</td>\n",
       "      <td>Dirac Operator on the Quantum Sphere</td>\n",
       "      <td>2001</td>\n",
       "      <td>A. PinzulA. Stern</td>\n",
       "      <td>We construct a Dirac operator on the quantum s...</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>dirac</td>\n",
       "      <td>dirac</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>8</td>\n",
       "      <td>cond-mat/0101228</td>\n",
       "      <td>Bounding and approximating parabolas for the s...</td>\n",
       "      <td>2001</td>\n",
       "      <td>H. -J. SchmidtJ. SchnackMarshall Luban</td>\n",
       "      <td>We prove that for a wide class of quantum spin...</td>\n",
       "      <td>0.822242</td>\n",
       "      <td>spin</td>\n",
       "      <td>bounds, spin</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>blk1|si0</td>\n",
       "      <td>The current work uses a range of methods to de...</td>\n",
       "      <td>uses</td>\n",
       "      <td>[Uses]</td>\n",
       "      <td>True</td>\n",
       "      <td>This sentence was generated to fulfill the 'Us...</td>\n",
       "      <td>9</td>\n",
       "      <td>math/0103058</td>\n",
       "      <td>A lower bound in an approximation problem invo...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Jean-Francois Burnol</td>\n",
       "      <td>We slightly improve the lower bound of Baez-Du...</td>\n",
       "      <td>0.821792</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-06T18:42:14.038301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx sentence_uid  \\\n",
       "0             0     blk1|si0   \n",
       "1             0     blk1|si0   \n",
       "2             0     blk1|si0   \n",
       "3             0     blk1|si0   \n",
       "4             0     blk1|si0   \n",
       "5             0     blk1|si0   \n",
       "6             0     blk1|si0   \n",
       "7             0     blk1|si0   \n",
       "8             0     blk1|si0   \n",
       "9             0     blk1|si0   \n",
       "\n",
       "                                       sentence_text citation_function  \\\n",
       "0  The current work uses a range of methods to de...              uses   \n",
       "1  The current work uses a range of methods to de...              uses   \n",
       "2  The current work uses a range of methods to de...              uses   \n",
       "3  The current work uses a range of methods to de...              uses   \n",
       "4  The current work uses a range of methods to de...              uses   \n",
       "5  The current work uses a range of methods to de...              uses   \n",
       "6  The current work uses a range of methods to de...              uses   \n",
       "7  The current work uses a range of methods to de...              uses   \n",
       "8  The current work uses a range of methods to de...              uses   \n",
       "9  The current work uses a range of methods to de...              uses   \n",
       "\n",
       "  citation_functions  needs_citation  \\\n",
       "0             [Uses]            True   \n",
       "1             [Uses]            True   \n",
       "2             [Uses]            True   \n",
       "3             [Uses]            True   \n",
       "4             [Uses]            True   \n",
       "5             [Uses]            True   \n",
       "6             [Uses]            True   \n",
       "7             [Uses]            True   \n",
       "8             [Uses]            True   \n",
       "9             [Uses]            True   \n",
       "\n",
       "                                       justification  rank          paper_id  \\\n",
       "0  This sentence was generated to fulfill the 'Us...     0      math/0104121   \n",
       "1  This sentence was generated to fulfill the 'Us...     1      math/0103095   \n",
       "2  This sentence was generated to fulfill the 'Us...     2      math/0101061   \n",
       "3  This sentence was generated to fulfill the 'Us...     3      math/0101111   \n",
       "4  This sentence was generated to fulfill the 'Us...     4      math/0102135   \n",
       "5  This sentence was generated to fulfill the 'Us...     5  quant-ph/0103089   \n",
       "6  This sentence was generated to fulfill the 'Us...     6      math/0101084   \n",
       "7  This sentence was generated to fulfill the 'Us...     7    hep-th/0103206   \n",
       "8  This sentence was generated to fulfill the 'Us...     8  cond-mat/0101228   \n",
       "9  This sentence was generated to fulfill the 'Us...     9      math/0103058   \n",
       "\n",
       "                                               title  year  \\\n",
       "0  Eigenvalue estimates of the Dirac operator dep...  2001   \n",
       "1     On eigenvalue estimates for the Dirac operator  2001   \n",
       "2                       Spectral estimates on 2-tori  2001   \n",
       "3  Eigenvalue estimates for the Dirac-Schr\\\"oding...  2001   \n",
       "4                      New bounds on Kakeya problems  2001   \n",
       "5  Geometrisation of electromagnetic field and to...  2001   \n",
       "6  Curvature Estimates in Asymptotically Flat Man...  2001   \n",
       "7               Dirac Operator on the Quantum Sphere  2001   \n",
       "8  Bounding and approximating parabolas for the s...  2001   \n",
       "9  A lower bound in an approximation problem invo...  2001   \n",
       "\n",
       "                                  authors  \\\n",
       "0  Thomas FriedrichKlaus-Dieter Kirchberg   \n",
       "1                       N. GinouxB. Morel   \n",
       "2                            Bernd Ammann   \n",
       "3                          Bertrand Morel   \n",
       "4                    Nets KatzTerence Tao   \n",
       "5                            O. A. Olkhov   \n",
       "6                  Felix FinsterInes Kath   \n",
       "7                       A. PinzulA. Stern   \n",
       "8  H. -J. SchmidtJ. SchnackMarshall Luban   \n",
       "9                    Jean-Francois Burnol   \n",
       "\n",
       "                                            abstract  normalized_score  \\\n",
       "0  We prove a new lower bound for the first eigen...          0.861814   \n",
       "1  We give lower bounds for the eigenvalues of th...          0.855258   \n",
       "2  We prove upper and lower bounds for the eigenv...          0.851397   \n",
       "3  We give new estimates for the eigenvalues of t...          0.843709   \n",
       "4  We establish new estimates on the Minkowski an...          0.830213   \n",
       "5  A new approach is proposed for an electromagne...          0.825784   \n",
       "6  We consider an asymptotically flat Riemannian ...          0.824458   \n",
       "7  We construct a Dirac operator on the quantum s...          0.822730   \n",
       "8  We prove that for a wide class of quantum spin...          0.822242   \n",
       "9  We slightly improve the lower bound of Baez-Du...          0.821792   \n",
       "\n",
       "                     title_matches  \\\n",
       "0                 dirac, estimates   \n",
       "1                 dirac, estimates   \n",
       "2                        estimates   \n",
       "3      dirac, estimates, operators   \n",
       "4                           bounds   \n",
       "5                                    \n",
       "6  curvature, estimates, manifolds   \n",
       "7                            dirac   \n",
       "8                             spin   \n",
       "9                                    \n",
       "\n",
       "                                         abs_matches retrieval_error  \\\n",
       "0  bounds, curvature, dirac, manifolds, riemannia...            None   \n",
       "1                bounds, curvature, dirac, operators            None   \n",
       "2              bounds, dirac, riemannian, spin, uses            None   \n",
       "3          curvature, dirac, estimates, inequalities            None   \n",
       "4                       bounds, establish, estimates            None   \n",
       "5                                   curvature, dirac            None   \n",
       "6                bounds, curvature, riemannian, spin            None   \n",
       "7                                              dirac            None   \n",
       "8                                       bounds, spin            None   \n",
       "9                                                               None   \n",
       "\n",
       "                 retrieved_at  \n",
       "0  2025-09-06T18:42:14.038301  \n",
       "1  2025-09-06T18:42:14.038301  \n",
       "2  2025-09-06T18:42:14.038301  \n",
       "3  2025-09-06T18:42:14.038301  \n",
       "4  2025-09-06T18:42:14.038301  \n",
       "5  2025-09-06T18:42:14.038301  \n",
       "6  2025-09-06T18:42:14.038301  \n",
       "7  2025-09-06T18:42:14.038301  \n",
       "8  2025-09-06T18:42:14.038301  \n",
       "9  2025-09-06T18:42:14.038301  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Run llm_test.py to regenerate the file (it overwrites classified_outputs.jsonl)\n",
    "\n",
    "# 2) Process the single block\n",
    "df_all = process_current_classification(\n",
    "    classified_path=\"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/classification_data/classified_outputs.jsonl\",\n",
    "    out_dir=\"outputs\",\n",
    "    topN=50,\n",
    "    topK_return=10,\n",
    "    normalize_scores=True,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "# 3) Inspect safely\n",
    "if df_all.empty:\n",
    "    print(\"No rows saved â€” check your classification or retrieval.\")\n",
    "else:\n",
    "    # Show the first sentence's candidates\n",
    "    s0 = df_all[\"sentence_idx\"].min()\n",
    "    display(df_all[(df_all[\"sentence_idx\"]==s0) & (df_all[\"rank\"].notna())].sort_values(\"rank\").head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2248accb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt sent to LLM ===\n",
      "You are a scientific evaluator. Given one clause and N candidate abstracts, decide which abstract (if any) best supports the clause for the specified citation function.\n",
      "Allowed functions: BACKGROUND, USE, COMPARE, EXTENDS, CONTINUATION, FUTUREWORK.\n",
      "Function to evaluate: COMPARE\n",
      "- BACKGROUND: definition/scene-setting.\n",
      "- USE: concrete application/implementation/where it's used.\n",
      "- COMPARE: explicit comparison, pros/cons, versus/baseline (look for phrases like 'compared to', 'versus', 'in contrast', 'baseline').\n",
      "- EXTENDS: builds on prior work, generalizes.\n",
      "- CONTINUATION: follow-up/replication/continued line.\n",
      "- FUTUREWORK: explicit future work/limitations/next steps.\n",
      "Strict accept rule:\n",
      "  â€¢ Only select a candidate if it (a) contains at least 2 topical overlap terms with the clause, AND (b) explicitly expresses the requested function.\n",
      "  â€¢ If no candidate satisfies both, return best_id=null and best_idx=null.\n",
      "Return STRICT JSON only (no prose) with keys:\n",
      "{\"best_id\": \"arxiv_id or null\", \"best_idx\": P# or null, \"confidence\": 0-1, \"per_candidate\": [{\"idx\": P#, \"arxiv_id\": \"...\", \"fits\": true/false, \"score\": 0-1, \"overlap_terms\": [\"...\"], \"has_function_cue\": true/false, \"rationale\": \"...\"}]}\n",
      "\n",
      "Clause:\n",
      "Compared to simple quench models, back reactions temper and delay amplification.\n",
      "\n",
      "Clause terms (topic anchors): amplification, back, compared, delay, models, quench, reactions, simple, temper\n",
      "\n",
      "Candidates:\n",
      "[P1] nucl-th/0104040 (prefix: nucl-th) â€” Microscopic Reaction Dynamics at SPS and RHIC\n",
      "The current status of transport theoretical models applicable to the physics\n",
      "of the Relativistic Heavy-Ion Collider is reviewed. The time evolution of\n",
      "microscopic reaction dynamics - from early, hard, partonic rescattering up to\n",
      "soft hadronic interactions close to freeze-out is analyzed and key observables\n",
      "linked to the different reaction stages are discussed.\n",
      "Hint: overlap_terms=['models']; has_function_cue=False\n",
      "[P2] nucl-th/0104066 (prefix: nucl-th) â€” Jet Quenching and the p-bar >= pi- Anomaly at RHIC\n",
      "PHENIX data on Au+Au at root(s)_= 130 AGeV suggest that p-bar yields may\n",
      "exceed pi- at high p_T > 2 GeV/c. We propose that jet quenching in central\n",
      "collisions suppresses the hard PQCD component of the spectra in central A+A\n",
      "reactions, thereby exposing a novel component of baryon dynamics that we\n",
      "attribute to (gluonic) baryon junctions. We predict that the observed p-bar >=\n",
      "pi- and the p > pi+ anomaly at p_T ~ 2 GeV/c is limited to a finite p_Tâ€¦\n",
      "Hint: overlap_terms=['reactions']; has_function_cue=False\n",
      "[P3] astro-ph/0103011 (prefix: astro-ph) â€” New results on the temporal structure of GRBs\n",
      "We analyze the temporal structure of long ((T_{90}>2sec)) and short\n",
      "((T_{90}<2sec)) BATSE bursts. We find that: (i) In many short bursts (\\delta\n",
      "t_{min}/T\\ll 1) (where (\\delta t_{min}) is the shortest pulse). This indicates\n",
      "that short bursts arise, like long ones, in internal shocks. (ii) In long\n",
      "bursts there is an excess of long intervals between pulses (relative to a\n",
      "lognormal distribut\n",
      "\n",
      "=== End prompt ===\n"
     ]
    }
   ],
   "source": [
    "prompt = build_llm_postfilter_prompt(\n",
    "    clause=clause,\n",
    "    func=func,\n",
    "    results=results,   # or candidates[:10] if you rerank\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(\"=== Prompt sent to LLM ===\")\n",
    "print(prompt[:3000])   # print first ~3000 chars so it's not overwhelming\n",
    "print(\"\\n=== End prompt ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
