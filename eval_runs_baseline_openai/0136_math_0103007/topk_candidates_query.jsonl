{"rank": 0, "score": 0.8619199395179749, "paper_id": "math/0103007", "title": "Source Coding, Large Deviations, and Approximate Pattern Matching", "abstract": "We present a development of parts of rate-distortion theory and pattern-\nmatching algorithms for lossy data compression, centered around a lossy version\nof the Asymptotic Equipartition Property (AEP). This treatment closely\nparallels the corresponding development in lossless compression, a point of\nview that was advanced in an important paper of Wyner and Ziv in 1989. In the\nlossless case we review how the AEP underlies the analysis of the Lempel-Ziv\nalgorithm by viewing it as a random code and reducing it to the idealized\nShannon code. This also provides information about the redundancy of the\nLempel-Ziv algorithm and about the asymptotic behavior of several relevant\nquantities. In the lossy case we give various versions of the statement of the\ngeneralized AEP and we outline the general methodology of its proof via large\ndeviations. Its relationship with Barron's generalized AEP is also discussed.\nThe lossy AEP is applied to: (i) prove strengthened versions of Shannon's\nsource coding theorem and universal coding theorems; (ii) characterize the\nperformance of mismatched codebooks; (iii) analyze the performance of pattern-\nmatching algorithms for lossy compression; (iv) determine the first order\nasymptotics of waiting times (with distortion) between stationary processes;\n(v) characterize the best achievable rate of weighted codebooks as an optimal\nsphere-covering exponent. We then present a refinement to the lossy AEP and use\nit to: (i) prove second order coding theorems; (ii) characterize which sources\nare easier to compress; (iii) determine the second order asymptotics of waiting\ntimes; (iv) determine the precise asymptotic behavior of longest match-lengths.\nExtensions to random fields are also given.", "abstract_full": "We present a development of parts of rate-distortion theory and pattern-\nmatching algorithms for lossy data compression, centered around a lossy version\nof the Asymptotic Equipartition Property (AEP). This treatment closely\nparallels the corresponding development in lossless compression, a point of\nview that was advanced in an important paper of Wyner and Ziv in 1989. In the\nlossless case we review how the AEP underlies the analysis of the Lempel-Ziv\nalgorithm by viewing it as a random code and reducing it to the idealized\nShannon code. This also provides information about the redundancy of the\nLempel-Ziv algorithm and about the asymptotic behavior of several relevant\nquantities. In the lossy case we give various versions of the statement of the\ngeneralized AEP and we outline the general methodology of its proof via large\ndeviations. Its relationship with Barron's generalized AEP is also discussed.\nThe lossy AEP is applied to: (i) prove strengthened versions of Shannon's\nsource coding theorem and universal coding theorems; (ii) characterize the\nperformance of mismatched codebooks; (iii) analyze the performance of pattern-\nmatching algorithms for lossy compression; (iv) determine the first order\nasymptotics of waiting times (with distortion) between stationary processes;\n(v) characterize the best achievable rate of weighted codebooks as an optimal\nsphere-covering exponent. We then present a refinement to the lossy AEP and use\nit to: (i) prove second order coding theorems; (ii) characterize which sources\nare easier to compress; (iii) determine the second order asymptotics of waiting\ntimes; (iv) determine the precise asymptotic behavior of longest match-lengths.\nExtensions to random fields are also given.", "authors": "A. Dembo, I. Kontoyiannis", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 1, "score": 0.8473485708236694, "paper_id": "physics/0104020", "title": "Information preserved guided scan pixel difference coding for medical\n  images", "abstract": "This paper analyzes the information content of medical images, with 3-D MRI\nimages as an example, in terms of information entropy. The results of the\nanalysis justify the use of Pixel Difference Coding for preserving all\ninformation contained in the original pictures, lossless coding in other words.\nThe experimental results also indicate that the compression ratio CR=2:1 can be\nachieved under the lossless constraints. A pratical implementation of Pixel\nDifference Coding which allows interactive retrieval of local ROI (Region of\nInterest), while maintaining the near low bound information entropy, is\ndiscussed.", "abstract_full": "This paper analyzes the information content of medical images, with 3-D MRI\nimages as an example, in terms of information entropy. The results of the\nanalysis justify the use of Pixel Difference Coding for preserving all\ninformation contained in the original pictures, lossless coding in other words.\nThe experimental results also indicate that the compression ratio CR=2:1 can be\nachieved under the lossless constraints. A pratical implementation of Pixel\nDifference Coding which allows interactive retrieval of local ROI (Region of\nInterest), while maintaining the near low bound information entropy, is\ndiscussed.", "authors": "Kunio Takaya, C. Tannous, Li Yuan", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 2, "score": 0.8395994901657104, "paper_id": "cond-mat/0101105", "title": "External losses in photoemission from strongly correlated quasi\n  two-dimensional solids", "abstract": "New expressions are derived for photoemission, which allow experimental\nelectron energy loss data to be used for estimating losses in photoemission.\nThe derivation builds on new results for dielectric response and mean free\npaths of strongly correlated systems of two dimensional layers. Numerical\nevaluations are made for $Bi_{2}Sr_{2}CaCu_{2}O_{8}$ (Bi2212) by using a\nparametrized loss function. The mean free path for Bi2212 is calculated and\nfound to be substantially larger than obtained by Norman et al in a recent\npaper. The photocurrent is expressed as the convolution of the intrinsic\napproximation for the current from a specific 2D layer with an effective loss\nfunction. The observed current is the sum of such currents from the first few\nlayers. The photo electron from a specific $CuO$ layer is found to excite low\nenergy acoustic plasmon modes due to the coupling between the $CuO$ layers.\nThese modes give rise to an asymmetric power law broadening of the photo\ncurrent an isolated two dimensional layer would have given. We define an\nasymmetry index where a contribution from a Luttinger lineshape is additive to\nthe contribution from our broadening function. Already the loss effect\nconsidered here gives broadening comparable to what is observed experimentally.\nA superconductor with a gapped loss function is predicted to have a\npeak-dip-hump lineshape similar to what has been observed, and with the same\nqualitative behavior as predicted in the recent work by Campuzano et al.", "abstract_full": "New expressions are derived for photoemission, which allow experimental\nelectron energy loss data to be used for estimating losses in photoemission.\nThe derivation builds on new results for dielectric response and mean free\npaths of strongly correlated systems of two dimensional layers. Numerical\nevaluations are made for $Bi_{2}Sr_{2}CaCu_{2}O_{8}$ (Bi2212) by using a\nparametrized loss function. The mean free path for Bi2212 is calculated and\nfound to be substantially larger than obtained by Norman et al in a recent\npaper. The photocurrent is expressed as the convolution of the intrinsic\napproximation for the current from a specific 2D layer with an effective loss\nfunction. The observed current is the sum of such currents from the first few\nlayers. The photo electron from a specific $CuO$ layer is found to excite low\nenergy acoustic plasmon modes due to the coupling between the $CuO$ layers.\nThese modes give rise to an asymmetric power law broadening of the photo\ncurrent an isolated two dimensional layer would have given. We define an\nasymmetry index where a contribution from a Luttinger lineshape is additive to\nthe contribution from our broadening function. Already the loss effect\nconsidered here gives broadening comparable to what is observed experimentally.\nA superconductor with a gapped loss function is predicted to have a\npeak-dip-hump lineshape similar to what has been observed, and with the same\nqualitative behavior as predicted in the recent work by Campuzano et al.", "authors": "L. Hedin, J. D. Lee", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 3, "score": 0.8391661047935486, "paper_id": "math/0103170", "title": "Minimizing Polynomial Functions", "abstract": "We compare algorithms for global optimization of polynomial functions in many\nvariables. It is demonstrated that existing algebraic methods (Gr\\\"obner bases,\nresultants, homotopy methods) are dramatically outperformed by a relaxation\ntechnique, due to N.Z. Shor and the first author, which involves sums of\nsquares and semidefinite programming. This opens up the possibility of using\nsemidefinite programming relaxations arising from the Positivstellensatz for a\nwide range of computational problems in real algebraic geometry.\n  This paper was presented at the Workshop on Algorithmic and Quantitative\nAspects of Real Algebraic Geometry in Mathematics and Computer Science, held at\nDIMACS, Rutgers University, March 12-16, 2001.", "abstract_full": "We compare algorithms for global optimization of polynomial functions in many\nvariables. It is demonstrated that existing algebraic methods (Gr\\\"obner bases,\nresultants, homotopy methods) are dramatically outperformed by a relaxation\ntechnique, due to N.Z. Shor and the first author, which involves sums of\nsquares and semidefinite programming. This opens up the possibility of using\nsemidefinite programming relaxations arising from the Positivstellensatz for a\nwide range of computational problems in real algebraic geometry.\n  This paper was presented at the Workshop on Algorithmic and Quantitative\nAspects of Real Algebraic Geometry in Mathematics and Computer Science, held at\nDIMACS, Rutgers University, March 12-16, 2001.", "authors": "Pablo A. Parrilo, Bernd Sturmfels", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 4, "score": 0.8387107849121094, "paper_id": "gr-qc/0104063", "title": "The Lazarus project: A pragmatic approach to binary black hole\n  evolutions", "abstract": "We present a detailed description of techniques developed to combine 3D\nnumerical simulations and, subsequently, a single black hole close-limit\napproximation. This method has made it possible to compute the first complete\nwaveforms covering the post-orbital dynamics of a binary black hole system with\nthe numerical simulation covering the essential non-linear interaction before\nthe close limit becomes applicable for the late time dynamics. To determine\nwhen close-limit perturbation theory is applicable we apply a combination of\ninvariant a priori estimates and a posteriori consistency checks of the\nrobustness of our results against exchange of linear and non-linear treatments\nnear the interface. Once the numerically modeled binary system reaches a regime\nthat can be treated as perturbations of the Kerr spacetime, we must\napproximately relate the numerical coordinates to the perturbative background\ncoordinates. We also perform a rotation of a numerically defined tetrad to\nasymptotically reproduce the tetrad required in the perturbative treatment. We\ncan then produce numerical Cauchy data for the close-limit evolution in the\nform of the Weyl scalar $\\psi_4$ and its time derivative $\\partial_t\\psi_4$\nwith both objects being first order coordinate and tetrad invariant. The\nTeukolsky equation in Boyer-Lindquist coordinates is adopted to further\ncontinue the evolution. To illustrate the application of these techniques we\nevolve a single Kerr hole and compute the spurious radiation as a measure of\nthe error of the whole procedure. We also briefly discuss the extension of the\nproject to make use of improved full numerical evolutions and outline the\napproach to a full understanding of astrophysical black hole binary systems\nwhich we can now pursue.", "abstract_full": "We present a detailed description of techniques developed to combine 3D\nnumerical simulations and, subsequently, a single black hole close-limit\napproximation. This method has made it possible to compute the first complete\nwaveforms covering the post-orbital dynamics of a binary black hole system with\nthe numerical simulation covering the essential non-linear interaction before\nthe close limit becomes applicable for the late time dynamics. To determine\nwhen close-limit perturbation theory is applicable we apply a combination of\ninvariant a priori estimates and a posteriori consistency checks of the\nrobustness of our results against exchange of linear and non-linear treatments\nnear the interface. Once the numerically modeled binary system reaches a regime\nthat can be treated as perturbations of the Kerr spacetime, we must\napproximately relate the numerical coordinates to the perturbative background\ncoordinates. We also perform a rotation of a numerically defined tetrad to\nasymptotically reproduce the tetrad required in the perturbative treatment. We\ncan then produce numerical Cauchy data for the close-limit evolution in the\nform of the Weyl scalar $\\psi_4$ and its time derivative $\\partial_t\\psi_4$\nwith both objects being first order coordinate and tetrad invariant. The\nTeukolsky equation in Boyer-Lindquist coordinates is adopted to further\ncontinue the evolution. To illustrate the application of these techniques we\nevolve a single Kerr hole and compute the spurious radiation as a measure of\nthe error of the whole procedure. We also briefly discuss the extension of the\nproject to make use of improved full numerical evolutions and outline the\napproach to a full understanding of astrophysical black hole binary systems\nwhich we can now pursue.", "authors": "J. Baker, M. Campanelli, C. Lousto", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 5, "score": 0.8379905223846436, "paper_id": "physics/0102066", "title": "A simple iterative algorithm for generating selected eigenspaces of\n  large matrices", "abstract": "We propose a new iterative algorithm for generating a subset of eigenvalues\nand eigenvectors of large matrices which generalizes the method of optimal\nrelaxations. We also give convergence criteria for the iterative process,\ninvestigate its efficiency by evaluating computer storage and time requirements\nand by a few numerical tests.", "abstract_full": "We propose a new iterative algorithm for generating a subset of eigenvalues\nand eigenvectors of large matrices which generalizes the method of optimal\nrelaxations. We also give convergence criteria for the iterative process,\ninvestigate its efficiency by evaluating computer storage and time requirements\nand by a few numerical tests.", "authors": "F. Andreozzi, A. Porrino, N. Lo Iudice", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 6, "score": 0.8379738330841064, "paper_id": "quant-ph/0102110", "title": "Addendum to \"Nonlinear quantum evolution with maximal entropy\n  production\"", "abstract": "The author calls attention to previous work with related results, which has\nescaped scrutiny before the publication of the article \"Nonlinear quantum\nevolution with maximal entropy production\", Phys.Rev.A63, 022105 (2001).", "abstract_full": "The author calls attention to previous work with related results, which has\nescaped scrutiny before the publication of the article \"Nonlinear quantum\nevolution with maximal entropy production\", Phys.Rev.A63, 022105 (2001).", "authors": "S. Gheorghiu-Svirschevski", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 7, "score": 0.8367388248443604, "paper_id": "cond-mat/0103455", "title": "An Analysis of the Quasicontinuum Method", "abstract": "The aim of this paper is to present a streamlined and fully three-dimensional\nversion of the quasicontinuum (QC) theory of Tadmor et al. and to analyze its\naccuracy and convergence characteristics. Specifically, we assess the effect of\nthe summation rules on accuracy; we determine the rate of convergence of the\nmethod in the presence of strong singularities, such as point loads; and we\nassess the effect of the refinement tolerance, which controls the rate at which\nnew nodes are inserted in the model, on the development of dislocation\nmicrostructures.", "abstract_full": "The aim of this paper is to present a streamlined and fully three-dimensional\nversion of the quasicontinuum (QC) theory of Tadmor et al. and to analyze its\naccuracy and convergence characteristics. Specifically, we assess the effect of\nthe summation rules on accuracy; we determine the rate of convergence of the\nmethod in the presence of strong singularities, such as point loads; and we\nassess the effect of the refinement tolerance, which controls the rate at which\nnew nodes are inserted in the model, on the development of dislocation\nmicrostructures.", "authors": "J. Knap, M. Ortiz", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 8, "score": 0.8362375497817993, "paper_id": "physics/0101058", "title": "Image reconstruction without prior information", "abstract": "A novel framework for designing image reconstruction algorithms for linear\nforward problems is proposed. The framework is based on the novel concept of\nconserving the information in the data during image reconstruction rather than\nsupplementing it with prior information. The framework offers an explanation as\nto why the popular reconstruction algorithms for MRI, CT and convolution are\ngenerally expressible as left invertible matrices. Also, the framework can be\nused to improve linear deconvolution and tackle such stubborn linear inverse\nproblems as the Laplace transform.", "abstract_full": "A novel framework for designing image reconstruction algorithms for linear\nforward problems is proposed. The framework is based on the novel concept of\nconserving the information in the data during image reconstruction rather than\nsupplementing it with prior information. The framework offers an explanation as\nto why the popular reconstruction algorithms for MRI, CT and convolution are\ngenerally expressible as left invertible matrices. Also, the framework can be\nused to improve linear deconvolution and tackle such stubborn linear inverse\nproblems as the Laplace transform.", "authors": "Keith S Cover", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
{"rank": 9, "score": 0.8360573053359985, "paper_id": "astro-ph/0103411", "title": "Primordial nucleosynthesis and hadronic decay of a massive particle with\n  a relatively short lifetime", "abstract": "In this paper we consider the effects on big bang nucleosynthesis (BBN) of\nthe hadronic decay of a long-lived massive particle. If high-energy hadrons are\nemitted near the BBN epoch ($t \\sim 10^{-2}$ -- $10^2 \\sec$), they\nextraordinarily inter-convert the background nucleons each other even after the\nfreeze-out time of the neutron to proton ratio. Then, produced light element\nabundances are changed, and that may result in a significant discrepancy\nbetween standard BBN and observations. Especially on the theoretical side, now\nwe can obtain a lot of experimental data of hadrons and simulate the hadronic\ndecay process executing the numerical code of the hadron fragmentation even in\nthe high energy region where we have no experimental data. Using the light\nelement abundances computed in the hadron-injection scenario, we derive a\nconstraint on properties of such a particle by comparing our theoretical\nresults with observations.", "abstract_full": "In this paper we consider the effects on big bang nucleosynthesis (BBN) of\nthe hadronic decay of a long-lived massive particle. If high-energy hadrons are\nemitted near the BBN epoch ($t \\sim 10^{-2}$ -- $10^2 \\sec$), they\nextraordinarily inter-convert the background nucleons each other even after the\nfreeze-out time of the neutron to proton ratio. Then, produced light element\nabundances are changed, and that may result in a significant discrepancy\nbetween standard BBN and observations. Especially on the theoretical side, now\nwe can obtain a lot of experimental data of hadrons and simulate the hadronic\ndecay process executing the numerical code of the hadron fragmentation even in\nthe high energy region where we have no experimental data. Using the light\nelement abundances computed in the hadron-injection scenario, we derive a\nconstraint on properties of such a particle by comparing our theoretical\nresults with observations.", "authors": "K. Kohri", "year": "2001", "retrieved_at": "2025-10-26T17:04:52"}
