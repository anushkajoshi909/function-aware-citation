âœ… Available Models:
1. alias-code
2. alias-reasoning
3. alias-image-generation
4. alias-vision
5. alias-ha
6. meta-llama/Llama-3.3-70B-Instruct
7. meta-llama/Llama-3.1-8B-Instruct
8. Qwen/Qwen2-VL-7B-Instruct
9. openGPT-X/Teuken-7B-instruct-research-v0.4
10. deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
11. Qwen/Qwen3-Coder-30B-A3B-Instruct
12. Qwen/Qwen3-Embedding-4B
13. stabilityai/stable-diffusion-3.5-large-turbo
14. black-forest-labs/FLUX.1-dev
15. openai/whisper-large-v3
16. Kokoro-82M
17. tts-1-hd
18. deepseek-ai/DeepSeek-R1
19. openai/gpt-oss-120b
20. meta-llama/Llama-4-Scout-17B-16E-Instruct

ðŸ‘‰ Select a model number (default = 1): 
âœ… Using model: openai/gpt-oss-120b

ðŸ” Enter a scientific query or topic: 
ðŸ”Ž Citation Function Classification:
{
  "citation_functions": [
    "Background"
  ],
  "justification": "Fallback: classifier did not return a valid object."
}

âœ… Saved to classified_outputs.jsonl
ðŸ”§ Ready for retrieval module (handled in a separate file).
[NbConvertApp] Converting notebook /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/Retreival_query_based.ipynb to script
[NbConvertApp] Writing 24671 bytes to /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/Retreival_query_based.py
Loaded model in 2.09 s
Warmup ok

Reading arXiv_src_0101_001.jsonl: 0it [00:00, ?it/s]
Reading arXiv_src_0101_001.jsonl: 1it [00:00,  4.25it/s]
Reading arXiv_src_0101_001.jsonl: 76it [00:00, 285.31it/s]
Reading arXiv_src_0101_001.jsonl: 229it [00:00, 720.19it/s]
Reading arXiv_src_0101_001.jsonl: 334it [00:00, 825.61it/s]
Reading arXiv_src_0101_001.jsonl: 432it [00:00, 755.31it/s]
Reading arXiv_src_0101_001.jsonl: 518it [00:00, 608.48it/s]
Reading arXiv_src_0101_001.jsonl: 590it [00:01, 604.99it/s]
Reading arXiv_src_0101_001.jsonl: 699it [00:01, 722.05it/s]
Reading arXiv_src_0101_001.jsonl: 800it [00:01, 795.90it/s]
Reading arXiv_src_0101_001.jsonl: 906it [00:01, 861.49it/s]
Reading arXiv_src_0101_001.jsonl: 1012it [00:01, 904.60it/s]
Reading arXiv_src_0101_001.jsonl: 1146it [00:01, 1020.44it/s]
Reading arXiv_src_0101_001.jsonl: 1282it [00:01, 1114.70it/s]
Reading arXiv_src_0101_001.jsonl: 1397it [00:01, 1098.72it/s]
Reading arXiv_src_0101_001.jsonl: 1530it [00:01, 1161.64it/s]
Reading arXiv_src_0101_001.jsonl: 1668it [00:01, 1220.17it/s]
Reading arXiv_src_0101_001.jsonl: 1792it [00:02, 1192.78it/s]
Reading arXiv_src_0101_001.jsonl: 1949it [00:02, 1298.99it/s]
Reading arXiv_src_0101_001.jsonl: 2124it [00:02, 1426.29it/s]
Reading arXiv_src_0101_001.jsonl: 2276it [00:02, 1452.03it/s]
Reading arXiv_src_0101_001.jsonl: 2319it [00:02, 975.25it/s] 

Reading arXiv_src_0102_001.jsonl: 0it [00:00, ?it/s]
Reading arXiv_src_0102_001.jsonl: 134it [00:00, 1326.84it/s]
Reading arXiv_src_0102_001.jsonl: 274it [00:00, 1368.31it/s]
Reading arXiv_src_0102_001.jsonl: 411it [00:00, 851.50it/s] 
Reading arXiv_src_0102_001.jsonl: 513it [00:00, 522.97it/s]
Reading arXiv_src_0102_001.jsonl: 592it [00:00, 553.81it/s]
Reading arXiv_src_0102_001.jsonl: 676it [00:01, 613.34it/s]
Reading arXiv_src_0102_001.jsonl: 794it [00:01, 742.51it/s]
Reading arXiv_src_0102_001.jsonl: 926it [00:01, 884.36it/s]
Reading arXiv_src_0102_001.jsonl: 1037it [00:01, 941.49it/s]
Reading arXiv_src_0102_001.jsonl: 1151it [00:01, 987.47it/s]
Reading arXiv_src_0102_001.jsonl: 1269it [00:01, 1040.05it/s]
Reading arXiv_src_0102_001.jsonl: 1415it [00:01, 1158.67it/s]
Reading arXiv_src_0102_001.jsonl: 1549it [00:01, 1209.82it/s]
Reading arXiv_src_0102_001.jsonl: 1682it [00:01, 1243.56it/s]
Reading arXiv_src_0102_001.jsonl: 1834it [00:01, 1324.01it/s]
Reading arXiv_src_0102_001.jsonl: 1984it [00:02, 1374.50it/s]
Reading arXiv_src_0102_001.jsonl: 2134it [00:02, 1410.69it/s]
Reading arXiv_src_0102_001.jsonl: 2192it [00:02, 1020.50it/s]

Reading arXiv_src_0103_001.jsonl: 0it [00:00, ?it/s]
Reading arXiv_src_0103_001.jsonl: 137it [00:00, 1364.41it/s]
Reading arXiv_src_0103_001.jsonl: 303it [00:00, 1533.92it/s]
Reading arXiv_src_0103_001.jsonl: 457it [00:00, 1127.84it/s]
Reading arXiv_src_0103_001.jsonl: 580it [00:00, 721.20it/s] 
Reading arXiv_src_0103_001.jsonl: 682it [00:00, 785.94it/s]
Reading arXiv_src_0103_001.jsonl: 777it [00:00, 818.55it/s]
Reading arXiv_src_0103_001.jsonl: 885it [00:00, 884.91it/s]
Reading arXiv_src_0103_001.jsonl: 999it [00:01, 951.82it/s]
Reading arXiv_src_0103_001.jsonl: 1107it [00:01, 987.05it/s]
Reading arXiv_src_0103_001.jsonl: 1228it [00:01, 1045.95it/s]
Reading arXiv_src_0103_001.jsonl: 1338it [00:01, 1053.44it/s]
Reading arXiv_src_0103_001.jsonl: 1488it [00:01, 1180.84it/s]
Reading arXiv_src_0103_001.jsonl: 1622it [00:01, 1227.29it/s]
Reading arXiv_src_0103_001.jsonl: 1769it [00:01, 1297.38it/s]
Reading arXiv_src_0103_001.jsonl: 1918it [00:01, 1353.19it/s]
Reading arXiv_src_0103_001.jsonl: 2055it [00:01, 1352.54it/s]
Reading arXiv_src_0103_001.jsonl: 2192it [00:01, 1352.00it/s]
Reading arXiv_src_0103_001.jsonl: 2348it [00:02, 1412.95it/s]
Reading arXiv_src_0103_001.jsonl: 2441it [00:02, 1144.43it/s]

Reading arXiv_src_0104_001.jsonl: 0it [00:00, ?it/s]
Reading arXiv_src_0104_001.jsonl: 1it [00:00,  5.40it/s]
Reading arXiv_src_0104_001.jsonl: 89it [00:00, 383.96it/s]
Reading arXiv_src_0104_001.jsonl: 252it [00:00, 857.59it/s]
Reading arXiv_src_0104_001.jsonl: 388it [00:00, 1033.07it/s]
Reading arXiv_src_0104_001.jsonl: 504it [00:00, 710.80it/s] 
Reading arXiv_src_0104_001.jsonl: 595it [00:01, 485.89it/s]
Reading arXiv_src_0104_001.jsonl: 705it [00:01, 595.81it/s]
Reading arXiv_src_0104_001.jsonl: 796it [00:01, 659.64it/s]
Reading arXiv_src_0104_001.jsonl: 899it [00:01, 742.07it/s]
Reading arXiv_src_0104_001.jsonl: 1027it [00:01, 871.97it/s]
Reading arXiv_src_0104_001.jsonl: 1140it [00:01, 936.74it/s]
Reading arXiv_src_0104_001.jsonl: 1265it [00:01, 1020.96it/s]
Reading arXiv_src_0104_001.jsonl: 1377it [00:01, 1024.92it/s]
Reading arXiv_src_0104_001.jsonl: 1510it [00:01, 1110.11it/s]
Reading arXiv_src_0104_001.jsonl: 1649it [00:01, 1182.29it/s]
Reading arXiv_src_0104_001.jsonl: 1784it [00:02, 1226.46it/s]
Reading arXiv_src_0104_001.jsonl: 1929it [00:02, 1290.99it/s]
Reading arXiv_src_0104_001.jsonl: 2089it [00:02, 1378.86it/s]
Reading arXiv_src_0104_001.jsonl: 2229it [00:02, 948.16it/s] 
Reading arXiv_src_0104_001.jsonl: 2310it [00:02, 888.21it/s]
Loaded 9262 unique papers
           paper_id  ...  year
0  quant-ph/0101147  ...  2001
1  quant-ph/0101145  ...  2001
2  quant-ph/0101144  ...  2001

[3 rows x 4 columns]
[debug] query: How does the development of lossy data compression algorithms in the paper relate to previous work and what methods or results from previous work does it utilize?
[debug] classifier labels: ['Background']
[debug] got 20 candidates
rows written: 20
- outputs/topk_candidates_query.jsonl
- outputs/topk_candidates_query.csv
   rank     score  ...                retrieved_at classifier_functions
0     0  1.000000  ...  2025-09-21T19:36:59.853753           Background
1     1  0.547271  ...  2025-09-21T19:36:59.853753           Background
2     2  0.306509  ...  2025-09-21T19:36:59.853753           Background
3     3  0.293043  ...  2025-09-21T19:36:59.853753           Background
4     4  0.278897  ...  2025-09-21T19:36:59.853753           Background
5     5  0.256518  ...  2025-09-21T19:36:59.853753           Background
6     6  0.256000  ...  2025-09-21T19:36:59.853753           Background
7     7  0.217628  ...  2025-09-21T19:36:59.853753           Background
8     8  0.202054  ...  2025-09-21T19:36:59.853753           Background
9     9  0.196454  ...  2025-09-21T19:36:59.853753           Background

[10 rows x 12 columns]
[21:37:06] main | args: Namespace(max_check=20, debug=True)
[21:37:06] read_last_jsonl | path= classified_outputs.jsonl
[21:37:06] read_last_jsonl | total_lines= 1
[21:37:06] main | query='How does the development of lossy data compression algorithms in the paper relate to previous work and what methods or results from previous work does it utilize?' | funcs=['Background']
[21:37:08] main | core_terms= ['algorithms', 'lossy data compression']
[21:37:08] load_topk_jsonl | path= /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/outputs/topk_candidates_query.jsonl
[21:37:08] load_topk_jsonl | rows= 20
[21:37:08] load_topk_jsonl | first: rank= 0 paper_id= math/0103007 title_snip='Source Coding, Large Deviations, and Approximate Pattern Matching' abs_len= 1731
[21:37:08] main | running function= Background
[21:37:08] select | function= Background | candidates_in= 20
[21:37:08] select | core_terms= ['algorithms', 'lossy data compression']
[21:37:08] select | no cosine prune; kept=20/20
[21:37:08] select | pool_size= 20 | ranks= [0, 3, 8, 12, 4, 11, 13, 14, 17, 1, 2, 5, 6, 7, 9, 10, 15, 16, 18, 19] | pids= ['math/0103007', 'math/0103170', 'physics/0101058', 'hep-ph/0101182', 'gr-qc/0104063', 'hep-ph/0101023', 'hep-ph/0101195', 'astro-ph/0101113', 'math/0101207', 'physics/0104020', 'cond-mat/0101105', 'physics/0102066', 'quant-ph/0102110', 'cond-mat/0103455', 'astro-ph/0103411', 'math/0103109', 'hep-ph/0101126', 'astro-ph/0102340', 'nucl-ex/0104018', 'math/0101118']
[21:37:08] select | verify strict 1/20
[21:37:08] verify | func= Background | rank= 0 | pid= math/0103007 | title_snip='Source Coding, Large Deviations, and Approximate Pattern Matching' | abs_len= 1731
[21:37:11] verify | result supports= True fit= 0.80 top= 0.90 invalid= â€” cue_hint= False
[21:37:11] select | verify strict 2/20
[21:37:11] verify | func= Background | rank= 3 | pid= math/0103170 | title_snip='Minimizing Polynomial Functions' | abs_len= 729
[21:37:14] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:14] select | verify strict 3/20
[21:37:14] verify | func= Background | rank= 8 | pid= physics/0101058 | title_snip='Image reconstruction without prior information' | abs_len= 577
[21:37:16] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:16] select | verify strict 4/20
[21:37:16] verify | func= Background | rank= 12 | pid= hep-ph/0101182 | title_snip='Wavelets and their use' | abs_len= 1279
[21:37:19] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:19] select | verify strict 5/20
[21:37:19] verify | func= Background | rank= 4 | pid= gr-qc/0104063 | title_snip='The Lazarus project: A pragmatic approach to binary black hole
  evoluâ€¦' | abs_len= 1769
[21:37:21] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:21] select | verify strict 6/20
[21:37:21] verify | func= Background | rank= 11 | pid= hep-ph/0101023 | title_snip='The Semileptonic Decays $B\to\pi l\nu$ and $D\to\pi l\nu$ from Latticeâ€¦' | abs_len= 673
[21:37:24] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:24] select | verify strict 7/20
[21:37:24] verify | func= Background | rank= 13 | pid= hep-ph/0101195 | title_snip='Deep Inelastic Scattering of Leptons and Hadrons in the QCD Parton Modâ€¦' | abs_len= 601
[21:37:26] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:26] select | verify strict 8/20
[21:37:26] verify | func= Background | rank= 14 | pid= astro-ph/0101113 | title_snip='On Local Approximations to the Nonlinear Evolution of Large-Scale
  Stâ€¦' | abs_len= 700
[21:37:29] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:29] select | verify strict 9/20
[21:37:29] verify | func= Background | rank= 17 | pid= math/0101207 | title_snip='From PDE Systems and Metrics to Generalized Field Theories' | abs_len= 578
[21:37:31] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:31] select | verify strict 10/20
[21:37:31] verify | func= Background | rank= 1 | pid= physics/0104020 | title_snip='Information preserved guided scan pixel difference coding for medical
â€¦' | abs_len= 616
[21:37:34] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:34] select | verify strict 11/20
[21:37:34] verify | func= Background | rank= 2 | pid= cond-mat/0101105 | title_snip='External losses in photoemission from strongly correlated quasi
  two-â€¦' | abs_len= 1498
[21:37:36] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:36] select | verify strict 12/20
[21:37:36] verify | func= Background | rank= 5 | pid= physics/0102066 | title_snip='A simple iterative algorithm for generating selected eigenspaces of
  â€¦' | abs_len= 334
[21:37:39] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:39] select | verify strict 13/20
[21:37:39] verify | func= Background | rank= 6 | pid= quant-ph/0102110 | title_snip='Addendum to "Nonlinear quantum evolution with maximal entropy
  producâ€¦' | abs_len= 222
[21:37:41] verify | result supports= False fit= 0.00 top= 0.00 invalid= â€” cue_hint= False
[21:37:41] select | verify strict 14/20
[21:37:41] verify | func= Background | rank= 7 | pid= cond-mat/0103455 | title_snip='An Analysis of the Quasicontinuum Method' | abs_len= 556
[21:37:44] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:44] select | verify strict 15/20
[21:37:44] verify | func= Background | rank= 9 | pid= astro-ph/0103411 | title_snip='Primordial nucleosynthesis and hadronic decay of a massive particle wiâ€¦' | abs_len= 936
[21:37:46] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:46] select | verify strict 16/20
[21:37:46] verify | func= Background | rank= 10 | pid= math/0103109 | title_snip='In search of an evolutionary coding style' | abs_len= 687
[21:37:49] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:49] select | verify strict 17/20
[21:37:49] verify | func= Background | rank= 15 | pid= hep-ph/0101126 | title_snip='Next-to-leading order mass effects in QCD Compton process of polarizedâ€¦' | abs_len= 300
[21:37:51] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:51] select | verify strict 18/20
[21:37:51] verify | func= Background | rank= 16 | pid= astro-ph/0102340 | title_snip='Computational Methods for Gravitational Lensing' | abs_len= 770
[21:37:54] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:54] select | verify strict 19/20
[21:37:54] verify | func= Background | rank= 18 | pid= nucl-ex/0104018 | title_snip='The event generator DECAY4 for simulation of double beta processes andâ€¦' | abs_len= 387
[21:37:56] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False
[21:37:56] select | verify strict 20/20
[21:37:56] verify | func= Background | rank= 19 | pid= math/0101118 | title_snip='Bilinear Estimates and Applications to Nonlinear Wave Equations' | abs_len= 628
[21:37:59] verify | result supports= False fit= 0.00 top= 0.00 invalid= no_quote cue_hint= False

================================================================================
FUNCTION: Background
QUERY   : How does the development of lossy data compression algorithms in the paper relate to previous work and what methods or results from previous work does it utilize?
- WINNER: math/0103007 | Source Coding, Large Deviations, and Approximate Pattern Matching
- QUOTE : algorithms for lossy data compression
- WHY   : sufficient
- ANSWER: The development of lossy data compression algorithms in the paper builds upon previous work, particularly the lossless compression treatment by Wyner and Ziv in 1989, and utilizes methods such as the Asymptotic Equipartition Property and large deviations (math/0103007).

ðŸ“„ Saved: outputs/function_selection_results.json
ðŸ“„ Saved: outputs/function_selection_results.jsonl
ðŸ“ Answers-only: outputs/function_answers.txt
-> /home/anpa439f/anaconda3/envs/myenv/bin/python3 /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/classifying_question.py  (cwd=/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration)
âœ… Saved to classified_outputs.jsonl
ðŸ”§ Ready for retrieval module (handled in a separate file).
-> jupyter nbconvert --to script /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/Retreival_query_based.ipynb  (cwd=/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration)
ðŸ©¹ Hardened exported script: Retreival_query_based.py
-> /home/anpa439f/anaconda3/envs/myenv/bin/python3 /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/Retreival_query_based.py  (cwd=/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration)
âœ… Top-k candidates: /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/outputs/topk_candidates_query.jsonl
ðŸ”— Symlinked: /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/outputs/topk_candidates_query.jsonl -> /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/outputs/topk_candidates_query.jsonl
ðŸ§© Patched final-stage script written to: function_based_final_patched.py
-> /home/anpa439f/anaconda3/envs/myenv/bin/python3 /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/function_based_final_patched.py --debug --max-check 20  (cwd=/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration)

ðŸŽ‰ Pipeline complete.
  â€¢ Classified file : /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/classified_outputs.jsonl
  â€¢ TopK (canonical): /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/outputs/topk_candidates_query.jsonl
  â€¢ Final outputs   : /data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/RetrievalAugmentedGeneration/outputs
