{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:20:53.024315Z",
     "iopub.status.busy": "2025-09-18T20:20:53.023969Z",
     "iopub.status.idle": "2025-09-18T20:21:10.147511Z",
     "shell.execute_reply": "2025-09-18T20:21:10.146694Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:10.150627Z",
     "iopub.status.busy": "2025-09-18T20:21:10.150159Z",
     "iopub.status.idle": "2025-09-18T20:21:10.154470Z",
     "shell.execute_reply": "2025-09-18T20:21:10.153922Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---- EDIT THESE PATHS IF NEEDED ----\n",
    "jsonl_files = [\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0101_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0102_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0103_001.jsonl\",\n",
    "    \"/data/horse/ws/anpa439f-Function_Retrieval_Citation/Research_Project/Corpus/processed_unarxive_extended_data/unarXive_01981221/01/arXiv_src_0104_001.jsonl\",\n",
    "]\n",
    "\n",
    "# Directory where we'll save the index + metadata\n",
    "out_dir = Path(\"e5_index_subset_1\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model to use: 'intfloat/e5-base-v2' is a good speed/quality trade-off\n",
    "E5_MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "\n",
    "# Limit (optional): set to None to index everything\n",
    "MAX_PAPERS = None  # e.g., 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:10.156078Z",
     "iopub.status.busy": "2025-09-18T20:21:10.155915Z",
     "iopub.status.idle": "2025-09-18T20:21:22.058581Z",
     "shell.execute_reply": "2025-09-18T20:21:22.057811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in 10.93 s\n",
      "Warmup ok\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”’ Force CPU + tame threads + use local cache to avoid any network / CUDA shenanigans\n",
    "import os, torch, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"          # force no-GPU path\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # quieter + safer in notebooks\n",
    "os.environ[\"HF_HOME\"] = \"./hf_cache\"             # local cache (no network)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "E5_MODEL_NAME = \"intfloat/e5-small-v2\"  # swap to e5-base-v2 later\n",
    "\n",
    "t0 = time.time()\n",
    "model = SentenceTransformer(E5_MODEL_NAME, device=\"cpu\", cache_folder=\"./hf_cache\")\n",
    "print(\"Loaded model in\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# warmup to avoid first-call lag\n",
    "_ = model.encode([\"query: warmup\"], normalize_embeddings=True)\n",
    "print(\"Warmup ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:22.061627Z",
     "iopub.status.busy": "2025-09-18T20:21:22.061446Z",
     "iopub.status.idle": "2025-09-18T20:21:32.144339Z",
     "shell.execute_reply": "2025-09-18T20:21:32.143254Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1it [00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 30it [00:00, 104.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 177it [00:00, 561.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 303it [00:00, 786.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 399it [00:00, 776.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 488it [00:00, 631.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 562it [00:01, 573.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 667it [00:01, 685.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 772it [00:01, 766.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 884it [00:01, 853.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 997it [00:01, 926.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1096it [00:01, 683.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1221it [00:01, 809.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1325it [00:01, 861.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1464it [00:01, 995.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1590it [00:02, 1063.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1723it [00:02, 1135.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 1843it [00:02, 1118.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 2017it [00:02, 1287.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 2178it [00:02, 1378.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0101_001.jsonl: 2319it [00:02, 890.34it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1it [00:00,  4.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 132it [00:00, 519.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 267it [00:00, 817.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 371it [00:00, 758.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 461it [00:00, 612.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 535it [00:00, 613.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 605it [00:01, 621.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 689it [00:01, 677.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 803it [00:01, 799.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 933it [00:01, 935.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1039it [00:01, 968.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1156it [00:01, 1022.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1270it [00:01, 1054.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1412it [00:01, 1158.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1540it [00:01, 1192.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1672it [00:01, 1228.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1808it [00:02, 1267.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 1953it [00:02, 1321.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 2103it [00:02, 1374.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0102_001.jsonl: 2192it [00:02, 963.88it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1it [00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 27it [00:00, 138.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 165it [00:00, 663.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 315it [00:00, 977.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 423it [00:00, 1008.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 527it [00:00, 673.87it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 610it [00:00, 648.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 712it [00:01, 736.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 796it [00:01, 747.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 897it [00:01, 816.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1004it [00:01, 879.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1118it [00:01, 950.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1232it [00:01, 998.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1341it [00:01, 1024.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1482it [00:01, 1136.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1608it [00:01, 1172.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1752it [00:01, 1247.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 1891it [00:02, 1288.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 2028it [00:02, 1311.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 2165it [00:02, 1328.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 2309it [00:02, 1359.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0103_001.jsonl: 2441it [00:02, 1007.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1it [00:00,  8.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 2it [00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 144it [00:00, 627.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 298it [00:00, 975.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 405it [00:00, 991.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 506it [00:00, 697.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 588it [00:01, 466.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 693it [00:01, 572.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 769it [00:01, 462.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 866it [00:01, 554.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 980it [00:01, 676.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1096it [00:01, 784.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1203it [00:01, 854.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1309it [00:01, 906.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1430it [00:02, 988.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1563it [00:02, 1082.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1688it [00:02, 1126.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1812it [00:02, 1158.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 1964it [00:02, 1263.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 2116it [00:02, 1338.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 2256it [00:02, 1354.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Reading arXiv_src_0104_001.jsonl: 2310it [00:02, 863.15it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9262 unique papers\n",
      "           paper_id                                              title  \\\n",
      "0  quant-ph/0101147               Radiation trapping in coherent media   \n",
      "1  quant-ph/0101145  Mimicking a Kerrlike medium in the dispersive ...   \n",
      "2  quant-ph/0101144  What is Possible Without Disturbing Partially ...   \n",
      "\n",
      "                                             authors  year  \n",
      "0  [A. B. Matsko, I. Novikova, M. O. Scully, G. R...  2001  \n",
      "1     [A. B. Klimov, L. L. Sanchez-Soto, J. Delgado]  2001  \n",
      "2                    [Masato Koashi, Nobuyuki Imoto]  2001  \n"
     ]
    }
   ],
   "source": [
    "# build_meta_with_authors.py (cell)\n",
    "import json, ast, re, os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "MAX_PAPERS = None  # or an int to truncate during dev\n",
    "\n",
    "_yr_re = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def best_year_from_obj(obj):\n",
    "    for key in (\"year\",\"published\",\"date\",\"update_date\",\"created\"):\n",
    "        if key in obj and obj[key]:\n",
    "            s = str(obj[key])\n",
    "            try:\n",
    "                y = int(s[:4])\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "            except Exception:\n",
    "                m = _yr_re.search(s)\n",
    "                if m: return int(m.group(0))\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    pid = obj.get(\"paper_id\") or md.get(\"id\") or obj.get(\"id\") or md.get(\"arxiv_id\") or obj.get(\"arxiv_id\")\n",
    "    if isinstance(pid, str) and \"/\" in pid:\n",
    "        try:\n",
    "            yy = int(pid.split(\"/\")[1][:2])\n",
    "            return 2000 + yy if yy < 50 else 1900 + yy\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def extract_title_abstract(obj):\n",
    "    title = None\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    title = md.get(\"title\") or obj.get(\"title\")\n",
    "    abstract = None\n",
    "    if isinstance(md.get(\"abstract\"), str):\n",
    "        abstract = md[\"abstract\"]\n",
    "    if not abstract and isinstance(obj.get(\"abstract\"), dict):\n",
    "        abstract = obj[\"abstract\"].get(\"text\")\n",
    "    if not abstract:\n",
    "        abstract = obj.get(\"abstract\")\n",
    "    return title, abstract\n",
    "\n",
    "def norm_raw_authors(raw):\n",
    "    if raw is None: return []\n",
    "    if isinstance(raw, (list, tuple)): return [str(x).strip() for x in raw if str(x).strip()]\n",
    "    s = str(raw).strip()\n",
    "    if not s: return []\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try: data = json.loads(s)\n",
    "        except Exception:\n",
    "            try: data = ast.literal_eval(s)\n",
    "            except Exception: data = None\n",
    "        if isinstance(data, list): return norm_raw_authors(data)\n",
    "    sep = \";\" if \";\" in s else \",\"\n",
    "    return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "\n",
    "def authors_from_parsed(ap):\n",
    "    out=[]\n",
    "    if isinstance(ap, list):\n",
    "        for it in ap:\n",
    "            if isinstance(it, dict):\n",
    "                nm=(\" \".join([it.get(\"first\",\"\"), it.get(\"last\",\"\")])).strip()\n",
    "            elif isinstance(it, (list,tuple)):\n",
    "                last=str(it[0]).strip() if len(it)>0 else \"\"\n",
    "                first=str(it[1]).strip() if len(it)>1 else \"\"\n",
    "                nm=(\" \".join([first,last])).strip()\n",
    "            else:\n",
    "                nm=str(it).strip()\n",
    "            if nm: out.append(nm)\n",
    "    return out\n",
    "\n",
    "def authors_from_obj(obj):\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    if \"authors_parsed\" in obj:\n",
    "        a = authors_from_parsed(obj[\"authors_parsed\"])\n",
    "        if a: return a\n",
    "    if \"authors_parsed\" in md:\n",
    "        a = authors_from_parsed(md[\"authors_parsed\"])\n",
    "        if a: return a\n",
    "    if \"authors\" in obj:\n",
    "        a = norm_raw_authors(obj[\"authors\"])\n",
    "        if a: return a\n",
    "    if \"authors\" in md:\n",
    "        a = norm_raw_authors(md[\"authors\"])\n",
    "        if a: return a\n",
    "    return []\n",
    "\n",
    "def get_pid(obj):\n",
    "    md = obj.get(\"metadata\") or {}\n",
    "    return obj.get(\"paper_id\") or md.get(\"id\") or obj.get(\"id\") or md.get(\"arxiv_id\") or obj.get(\"arxiv_id\")\n",
    "\n",
    "rows = []\n",
    "for path in jsonl_files:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Reading {os.path.basename(path)}\"):\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            pid = get_pid(obj)\n",
    "            title, abstract = extract_title_abstract(obj)\n",
    "            if not pid or not title or not abstract:\n",
    "                continue\n",
    "            authors = authors_from_obj(obj)\n",
    "            year = best_year_from_obj(obj)\n",
    "            rows.append({\n",
    "                \"paper_id\": pid,\n",
    "                \"title\": title.strip(),\n",
    "                \"abstract\": str(abstract).strip(),\n",
    "                \"authors\": authors,\n",
    "                \"year\": year\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df.drop_duplicates(subset=[\"paper_id\"]).reset_index(drop=True)\n",
    "if MAX_PAPERS: df = df.head(MAX_PAPERS)\n",
    "print(f\"Loaded {len(df)} unique papers\")\n",
    "print(df.head(3)[[\"paper_id\",\"title\",\"authors\",\"year\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:32.146596Z",
     "iopub.status.busy": "2025-09-18T20:21:32.146420Z",
     "iopub.status.idle": "2025-09-18T20:21:34.655150Z",
     "shell.execute_reply": "2025-09-18T20:21:34.654320Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Reload index & metadata\n",
    "index = faiss.read_index(str(out_dir / \"index.faiss\"))\n",
    "meta  = pd.read_parquet(out_dir / \"meta.parquet\")\n",
    "\n",
    "# âœ… Reuse the already-loaded model from earlier\n",
    "q_model = model                      # <-- do NOT call SentenceTransformer() again\n",
    "_ = q_model.encode([\"query: warmup\"], normalize_embeddings=True)  # quick warmup\n",
    "\n",
    "def encode_query(q: str):\n",
    "    return q_model.encode([f\"query: {q}\"], normalize_embeddings=True).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:34.752599Z",
     "iopub.status.busy": "2025-09-18T20:21:34.752126Z",
     "iopub.status.idle": "2025-09-18T20:21:34.878221Z",
     "shell.execute_reply": "2025-09-18T20:21:34.877617Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# helpers (format/display)\n",
    "# -----------------------\n",
    "\n",
    "def _trim(text, max_chars=450):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = str(text).strip()\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    cut = s[:max_chars].rsplit(\" \", 1)[0]\n",
    "    return cut + \"â€¦\"\n",
    "\n",
    "\n",
    "def _format_authors(a, k=3):\n",
    "    if a is None:\n",
    "        return []\n",
    "    if isinstance(a, (list, tuple)):\n",
    "        names = [str(x).strip() for x in a if str(x).strip()]\n",
    "    else:\n",
    "        sep = \";\" if \";\" in str(a) else \",\"\n",
    "        names = [t.strip() for t in str(a).split(sep) if t.strip()]\n",
    "    if not names:\n",
    "        return []\n",
    "    return names[:k] + ([\"et al.\"] if len(names) > k else [])\n",
    "\n",
    "\n",
    "_yr_re = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def _best_year(row):\n",
    "    \"\"\"Robustly extract a plausible year (int) from heterogeneous row fields without ambiguous truth checks.\"\"\"\n",
    "    def _first_scalar(x):\n",
    "        if isinstance(x, (list, tuple, np.ndarray, pd.Series)):\n",
    "            return x[0] if len(x) > 0 else None\n",
    "        return x\n",
    "\n",
    "    for key in (\"year\", \"published\", \"date\", \"update_date\", \"created\"):\n",
    "        if key in row:\n",
    "            val = _first_scalar(row.get(key))\n",
    "            if val is None:\n",
    "                continue\n",
    "            s = str(val)\n",
    "            # try first 4 chars\n",
    "            try:\n",
    "                y = int(s[:4])\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "            except Exception:\n",
    "                pass\n",
    "            # regex fallback anywhere in the string\n",
    "            m = _yr_re.search(s)\n",
    "            if m:\n",
    "                y = int(m.group(0))\n",
    "                if 1900 <= y <= datetime.now().year + 1:\n",
    "                    return y\n",
    "\n",
    "    # last resort: parse from paper_id-like strings\n",
    "    pid = row.get(\"paper_id\") or row.get(\"arxiv_id\") or row.get(\"id\")\n",
    "    if isinstance(pid, str) and \"/\" in pid:\n",
    "        try:\n",
    "            yy = int(pid.split(\"/\")[1][:2])\n",
    "            return 2000 + yy if yy < 50 else 1900 + yy\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _extract_abstract(abstract_field):\n",
    "    if abstract_field is None:\n",
    "        return \"\"\n",
    "    if isinstance(abstract_field, dict):\n",
    "        return str(abstract_field.get(\"text\") or abstract_field.get(\"abstract\") or \"\")\n",
    "    return str(abstract_field)\n",
    "\n",
    "# -----------------------\n",
    "# stopwords + token utils\n",
    "# -----------------------\n",
    "\n",
    "DEFAULT_STOPWORDS = {\n",
    "    \"a\",\"an\",\"and\",\"the\",\"of\",\"to\",\"in\",\"on\",\"for\",\"with\",\"by\",\"as\",\"at\",\"or\",\"but\",\"if\",\"than\",\"then\",\n",
    "    \"from\",\"into\",\"over\",\"under\",\"between\",\"within\",\"without\",\"about\",\"via\",\"per\",\"through\",\"across\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"do\",\"does\",\"did\",\"can\",\"could\",\n",
    "    \"may\",\"might\",\"will\",\"would\",\"shall\",\"should\",\"must\",\"not\",\"no\",\"nor\",\"also\",\"both\",\"either\",\"neither\",\n",
    "    \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"their\",\"our\",\"your\",\"his\",\"her\",\"them\",\"they\",\"we\",\"you\",\"i\",\n",
    "    \"such\",\"thus\",\"there\",\"here\",\"where\",\"when\",\"which\",\"who\",\"whom\",\"whose\",\"what\",\"why\",\"how\",\n",
    "    \"using\",\"use\",\"used\",\"based\",\"approach\",\"approaches\",\"method\",\"methods\",\"result\",\"results\",\"show\",\n",
    "    \"shows\",\"shown\",\"paper\",\"study\",\"work\",\"new\"\n",
    "}\n",
    "\n",
    "def minmax_norm(x):\n",
    "    x = np.asarray(x, dtype=np.float32).reshape(-1)\n",
    "    if x.size == 0:\n",
    "        return x\n",
    "    lo, hi = float(np.min(x)), float(np.max(x))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or (hi - lo) < 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - lo) / (hi - lo)\n",
    "\n",
    "_token_re = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return [t.lower() for t in _token_re.findall(str(text))]\n",
    "\n",
    "def content_terms(tokens, stopwords, min_len=3):\n",
    "    return [t for t in tokens if not t.isdigit() and len(t) >= min_len and t not in stopwords]\n",
    "\n",
    "# -----------------------\n",
    "# robust author extraction\n",
    "# -----------------------\n",
    "\n",
    "def _authors_from_row(row):\n",
    "\n",
    "    def _norm_raw_authors(raw):\n",
    "        if raw is None:\n",
    "            return []\n",
    "        if isinstance(raw, (list, tuple)):\n",
    "            return [str(x).strip() for x in raw if str(x).strip()]\n",
    "        s = str(raw).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                data = json.loads(s)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "            if isinstance(data, list):\n",
    "                return _norm_raw_authors(data)\n",
    "        sep = \";\" if \";\" in s else \",\"\n",
    "        return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "\n",
    "    def _norm_authors_parsed(ap):\n",
    "        out = []\n",
    "        if isinstance(ap, (list, tuple)):\n",
    "            for item in ap:\n",
    "                if isinstance(item, (list, tuple)):\n",
    "                    last = str(item[0]).strip() if len(item) > 0 else \"\"\n",
    "                    first = str(item[1]).strip() if len(item) > 1 else \"\"\n",
    "                    name = \" \".join([first, last]).strip()\n",
    "                    if name:\n",
    "                        out.append(name)\n",
    "                elif isinstance(item, dict):\n",
    "                    first = str(item.get(\"first\", \"\")).strip()\n",
    "                    last = str(item.get(\"last\", \"\")).strip()\n",
    "                    name = \" \".join([first, last]).strip()\n",
    "                    if name:\n",
    "                        out.append(name)\n",
    "                else:\n",
    "                    s = str(item).strip()\n",
    "                    if s:\n",
    "                        out.append(s)\n",
    "        elif isinstance(ap, str) and ap.strip():\n",
    "            try:\n",
    "                data = json.loads(ap)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(ap)\n",
    "                except Exception:\n",
    "                    data = None\n",
    "            if isinstance(data, list):\n",
    "                return _norm_authors_parsed(data)\n",
    "        return out\n",
    "\n",
    "    # avoid pd.notna() on non-scalars â€” just check for presence/non-empty\n",
    "    if \"authors\" in row:\n",
    "        names = _norm_raw_authors(row.get(\"authors\"))\n",
    "        if names:\n",
    "            return names\n",
    "\n",
    "    if \"authors_parsed\" in row:\n",
    "        names = _norm_authors_parsed(row.get(\"authors_parsed\"))\n",
    "        if names:\n",
    "            return names\n",
    "\n",
    "    # nested metadata dict (JSONL)\n",
    "    if \"metadata\" in row and isinstance(row.get(\"metadata\"), dict):\n",
    "        md = row[\"metadata\"]\n",
    "        if \"authors\" in md:\n",
    "            names = _norm_raw_authors(md.get(\"authors\"))\n",
    "            if names:\n",
    "                return names\n",
    "        if \"authors_parsed\" in md:\n",
    "            names = _norm_authors_parsed(md.get(\"authors_parsed\"))\n",
    "            if names:\n",
    "                return names\n",
    "\n",
    "    return []\n",
    "\n",
    "# -----------------------\n",
    "# meta building from JSONL\n",
    "# -----------------------\n",
    "\n",
    "def _rows_from_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            md = obj.get(\"metadata\", {}) or {}\n",
    "            authors_field = md.get(\"authors\") or obj.get(\"authors\")\n",
    "            yield {\n",
    "                \"paper_id\": obj.get(\"paper_id\") or md.get(\"id\"),\n",
    "                \"title\": md.get(\"title\", \"\") or obj.get(\"title\", \"\"),\n",
    "                \"abstract\": obj.get(\"abstract\", md.get(\"abstract\", \"\")),\n",
    "                \"authors\": authors_field,\n",
    "                \"metadata\": md,\n",
    "            }\n",
    "\n",
    "def build_meta_from_jsonl(paths):\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "    all_rows = []\n",
    "    for p in paths:\n",
    "        for row in _rows_from_jsonl(p):\n",
    "            all_rows.append(row)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if \"paper_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"paper_id\"], keep=\"first\")\n",
    "    if \"title\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# main search function\n",
    "# -----------------------\n",
    "\n",
    "# COSINE-ONLY retrieval (query-only)\n",
    "def search_post_filter(\n",
    "    query,\n",
    "    topN=20,\n",
    "    topK_return=10,\n",
    "    normalize_scores=False,\n",
    "    stopwords=None,\n",
    "    min_term_len=3,\n",
    "    abstract_chars=450,\n",
    "    authors_shown=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Requires globals: index (faiss index), meta (DataFrame aligned to index), encode_query (-> np.float32 [1,d])\n",
    "    \"\"\"\n",
    "    stopwords = DEFAULT_STOPWORDS if stopwords is None else set(stopwords)\n",
    "\n",
    "    # 1) semantic retrieve\n",
    "    qv = encode_query(query)  # shape (1, d)\n",
    "    scores, idxs = index.search(qv, int(topN))  # shapes (1, topN)\n",
    "    scores, idxs = scores[0].astype(np.float32), idxs[0]\n",
    "\n",
    "    # 2) order purely by cosine\n",
    "    order = np.argsort(-scores)\n",
    "    display_scores = minmax_norm(scores) if normalize_scores else scores\n",
    "\n",
    "    # 3) lexical explainers\n",
    "    q_terms_all  = tokenize(query)\n",
    "    q_terms_used = content_terms(q_terms_all, stopwords, min_len=min_term_len)\n",
    "\n",
    "    out = []\n",
    "    limit = min(int(topK_return), len(order))\n",
    "    for rank_pos in range(limit):\n",
    "        r = order[rank_pos]\n",
    "        if r < 0 or r >= len(idxs):\n",
    "            continue  # safety\n",
    "        row = meta.iloc[idxs[r]]\n",
    "\n",
    "        title = row.get(\"title\", \"\")\n",
    "        abstract_txt = _extract_abstract(row.get(\"abstract\", \"\"))\n",
    "        paper_id = row.get(\"paper_id\") or row.get(\"arxiv_id\") or row.get(\"id\")\n",
    "\n",
    "        title_tokens = content_terms(tokenize(title), stopwords, min_len=min_term_len)\n",
    "        abs_tokens   = content_terms(tokenize(abstract_txt), stopwords, min_len=min_term_len)\n",
    "\n",
    "        title_matches = sorted(set(q_terms_used) & set(title_tokens))\n",
    "        abs_matches   = sorted(set(q_terms_used) & set(abs_tokens))\n",
    "\n",
    "        authors_list = _authors_from_row(row)\n",
    "        authors_fmt  = _format_authors(authors_list, k=authors_shown)\n",
    "\n",
    "        out.append({\n",
    "            \"score\": float(display_scores[r]),\n",
    "            \"cosine\": float(scores[r]),\n",
    "            \"title\": title,\n",
    "            \"abstract\": _trim(abstract_txt, abstract_chars),  # preview\n",
    "            \"abstract_full\": abstract_txt,                    # full\n",
    "            \"arxiv_id\": paper_id,\n",
    "            \"year\": _best_year(row),\n",
    "            \"authors\": authors_fmt,\n",
    "            \"title_matches\": title_matches,\n",
    "            \"abs_matches\": abs_matches,\n",
    "            \"query_terms_used\": q_terms_used,\n",
    "            # removed 'function_requested' (no desired_function in signature anymore)\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# convenience: results -> DataFrame\n",
    "# -----------------------\n",
    "\n",
    "def to_df(res):\n",
    "    cols = [\"cosine\",\"title\",\"year\",\"arxiv_id\",\"authors\",\"title_matches\",\"abs_matches\"]\n",
    "    return pd.DataFrame([{k: r.get(k) for k in cols} for r in res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:34.879946Z",
     "iopub.status.busy": "2025-09-18T20:21:34.879761Z",
     "iopub.status.idle": "2025-09-18T20:21:34.884582Z",
     "shell.execute_reply": "2025-09-18T20:21:34.884041Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def _safe_join(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, list):\n",
    "        return \", \".join(str(t) for t in x)\n",
    "    return str(x)\n",
    "\n",
    "def display_results_table(results):\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            \"Title\": r.get(\"title\", \"\"),\n",
    "            \"Year\": r.get(\"year\", \"\"),\n",
    "            \"Authors\": _safe_join(r.get(\"authors\", [])),\n",
    "            \"Abstract\": r.get(\"abstract\", \"\"),  # already trimmed upstream\n",
    "            \"Title Matches\": _safe_join(r.get(\"title_matches\", [])),\n",
    "            \"Abstract Matches\": _safe_join(r.get(\"abs_matches\", [])),\n",
    "            \"Score (norm)\": r.get(\"score\", r.get(\"cosine\", None)),\n",
    "            \"Cosine (raw)\": r.get(\"cosine\", None),\n",
    "        } for r in (results or [])\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:34.886333Z",
     "iopub.status.busy": "2025-09-18T20:21:34.886172Z",
     "iopub.status.idle": "2025-09-18T20:21:34.900255Z",
     "shell.execute_reply": "2025-09-18T20:21:34.899748Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_current_classification(\n",
    "    classified_path=\"classified_outputs.jsonl\",\n",
    "    out_dir=\"outputs\",\n",
    "    topN=50,\n",
    "    topK_return=10,\n",
    "    normalize_scores=True,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Read the latest query from `classified_outputs.jsonl`, run query-only retrieval\n",
    "    via `search(...)`, and write top-k candidates to JSONL and CSV.\n",
    "\n",
    "    - No sentence-level loop\n",
    "    - No answer/explanations\n",
    "    - Prefers full abstract if present in results (abstract_full), else falls back to abstract\n",
    "    - Robust against numpy arrays to avoid: \"The truth value of an array with more than one element is ambiguous\"\n",
    "    \"\"\"\n",
    "    import os, json\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "\n",
    "    def read_last_jsonl(path: str):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        if not lines:\n",
    "            raise ValueError(f\"No lines found in {path}\")\n",
    "        try:\n",
    "            return json.loads(lines[-1])\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Last line is not valid JSON: {e}\\nLine: {lines[-1][:200]}...\")\n",
    "\n",
    "    def as_list(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        return [x]\n",
    "\n",
    "    def join_if_list(x, sep=\", \"):\n",
    "        if isinstance(x, list):\n",
    "            return sep.join(str(t) for t in x)\n",
    "        return \"\" if x is None else str(x)\n",
    "\n",
    "    def safe_year(y):\n",
    "        try:\n",
    "            # Avoid numpy types/arrays ambiguity\n",
    "            if isinstance(y, (list, tuple)):\n",
    "                y = y[0] if y else None\n",
    "            s = str(y).strip()\n",
    "            if not s:\n",
    "                return None\n",
    "            return int(s[:4])\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_jsonl = os.path.join(out_dir, \"topk_candidates_query.jsonl\")\n",
    "    out_csv   = os.path.join(out_dir, \"topk_candidates_query.csv\")\n",
    "\n",
    "    obj = read_last_jsonl(classified_path)\n",
    "\n",
    "    query = (obj.get(\"query\") or \"\").strip()\n",
    "    if not query:\n",
    "        raise ValueError(\"No 'query' found in the latest classified_outputs.jsonl entry.\")\n",
    "\n",
    "    # provenance of classifier labels (not used for ranking)\n",
    "    cls = obj.get(\"citation_function_classification\") or {}\n",
    "    cls_funcs = cls.get(\"citation_functions\") or []\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[debug] query: {query}\")\n",
    "        if cls_funcs:\n",
    "            print(f\"[debug] classifier labels: {cls_funcs}\")\n",
    "\n",
    "    retrieved_at = datetime.utcnow().isoformat()\n",
    "\n",
    "    # ---- Query-only retrieval (use search; fall back to search_post_filter if needed) ----\n",
    "    retrieval_fn = globals().get(\"search\")\n",
    "    if retrieval_fn is None:\n",
    "        retrieval_fn = globals().get(\"search_post_filter\")\n",
    "    if retrieval_fn is None:\n",
    "        raise RuntimeError(\"Neither 'search' nor 'search_post_filter' is defined in the current scope.\")\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        # Ensure we always get a Python list (not a numpy array)\n",
    "        res = retrieval_fn(\n",
    "            query=query,\n",
    "            topN=int(topN),\n",
    "            topK_return=int(topK_return),\n",
    "            normalize_scores=bool(normalize_scores)\n",
    "        )\n",
    "        if isinstance(res, list):\n",
    "            results = res\n",
    "        elif res is None:\n",
    "            results = []\n",
    "        else:\n",
    "            # Defensive: convert iterables to list\n",
    "            try:\n",
    "                results = list(res)\n",
    "            except Exception:\n",
    "                results = []\n",
    "        if debug:\n",
    "            print(f\"[debug] got {len(results)} candidates\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"[debug] retrieval error: {e}\")\n",
    "        results = []\n",
    "\n",
    "    # ---- Flatten results for export ----\n",
    "    rows = []\n",
    "    for rank, r in enumerate(results):\n",
    "        # prefer full abstract if present in retrieval output\n",
    "        abs_full = r.get(\"abstract_full\")\n",
    "        if abs_full is None or not str(abs_full).strip():\n",
    "            abs_full = r.get(\"abstract\", \"\")  # fallback\n",
    "\n",
    "        rows.append({\n",
    "            \"rank\": int(rank),\n",
    "            \"paper_id\": r.get(\"arxiv_id\") or r.get(\"paper_id\") or \"\",\n",
    "            \"title\": r.get(\"title\", \"\"),\n",
    "            \"year\": safe_year(r.get(\"year\")),\n",
    "            \"authors\": join_if_list(r.get(\"authors\")),\n",
    "            \"abstract\": str(abs_full or \"\"),\n",
    "            \"score\": (float(r.get(\"score\")) if r.get(\"score\") is not None else None),   # normalized if provided\n",
    "            \"cosine\": (float(r.get(\"cosine\")) if r.get(\"cosine\") is not None else None),\n",
    "            \"title_matches\": join_if_list(r.get(\"title_matches\")),\n",
    "            \"abs_matches\": join_if_list(r.get(\"abs_matches\")),\n",
    "            \"query_terms_used\": join_if_list(r.get(\"query_terms_used\")),\n",
    "            \"classifier_functions\": join_if_list(as_list(cls_funcs)),\n",
    "            \"retrieval_error\": None,\n",
    "            \"retrieved_at\": retrieved_at\n",
    "        })\n",
    "\n",
    "    # If no results, emit a stub row so downstream doesnâ€™t break\n",
    "    if len(rows) == 0:\n",
    "        rows.append({\n",
    "            \"rank\": None,\n",
    "            \"paper_id\": \"\",\n",
    "            \"title\": \"\",\n",
    "            \"year\": None,\n",
    "            \"authors\": \"\",\n",
    "            \"abstract\": \"\",\n",
    "            \"score\": None,\n",
    "            \"cosine\": None,\n",
    "            \"title_matches\": \"\",\n",
    "            \"abs_matches\": \"\",\n",
    "            \"query_terms_used\": \"\",\n",
    "            \"classifier_functions\": join_if_list(as_list(cls_funcs)),\n",
    "            \"retrieval_error\": \"no_results\",\n",
    "            \"retrieved_at\": retrieved_at\n",
    "        })\n",
    "\n",
    "    # ---- Write outputs (overwrite each run) ----\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in rows:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"rows written: {len(rows)}\")\n",
    "        print(f\"- {out_jsonl}\\n- {out_csv}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T20:21:34.901933Z",
     "iopub.status.busy": "2025-09-18T20:21:34.901765Z",
     "iopub.status.idle": "2025-09-18T20:21:35.595291Z",
     "shell.execute_reply": "2025-09-18T20:21:35.594797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] query: How does the observation of anti-flow signal in neutral strange mesons motivate and extend our understanding of in-medium kaon vector potential in high density nuclear matter?\n",
      "[debug] classifier labels: ['Motivation', 'Extends']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] got 20 candidates\n",
      "rows written: 20\n",
      "- outputs/topk_candidates_query.jsonl\n",
      "- outputs/topk_candidates_query.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>cosine</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title_matches</th>\n",
       "      <th>abs_matches</th>\n",
       "      <th>query_terms_used</th>\n",
       "      <th>retrieved_at</th>\n",
       "      <th>classifier_functions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906184</td>\n",
       "      <td>Anti-flow of K$^0_s$ Mesons in 6 AGeV Au + Au ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>P. ChungN. N. AjitanandJ. M. AlexanderM. Ander...</td>\n",
       "      <td>nucl-ex/0101003</td>\n",
       "      <td>anti, flow, mesons</td>\n",
       "      <td>anti, density, flow, high, kaon, matter, mediu...</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.359875</td>\n",
       "      <td>0.872047</td>\n",
       "      <td>High p_T Higgs signal for the LHC</td>\n",
       "      <td>2001</td>\n",
       "      <td>V. A. KhozeA. D. MartinM. G. Ryskin</td>\n",
       "      <td>hep-ph/0104230</td>\n",
       "      <td>high, signal</td>\n",
       "      <td>anti, signal</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.357546</td>\n",
       "      <td>0.871923</td>\n",
       "      <td>Production and collective behavior of strange ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>C. Pinkenburg</td>\n",
       "      <td>nucl-ex/0104025</td>\n",
       "      <td>strange</td>\n",
       "      <td>anti, flow, mesons, neutral, signal, strange</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.291613</td>\n",
       "      <td>0.868406</td>\n",
       "      <td>Polarization Insights for Active Galactic Nuclei</td>\n",
       "      <td>2001</td>\n",
       "      <td>R. R. J. Antonucci</td>\n",
       "      <td>astro-ph/0103048</td>\n",
       "      <td></td>\n",
       "      <td>high, understanding</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.277690</td>\n",
       "      <td>0.867664</td>\n",
       "      <td>Exploring New Physics in the $B\\to \\phi K$ System</td>\n",
       "      <td>2001</td>\n",
       "      <td>Robert FleischerThomas Mannel</td>\n",
       "      <td>hep-ph/0103121</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.246621</td>\n",
       "      <td>0.866007</td>\n",
       "      <td>Asymmetries in phi photoproduction and the OZI...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Yongseok OhH. C. Bhang</td>\n",
       "      <td>nucl-th/0104068</td>\n",
       "      <td></td>\n",
       "      <td>density, vector</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.240123</td>\n",
       "      <td>0.865661</td>\n",
       "      <td>Antikaons in nuclei and dense nuclear matter</td>\n",
       "      <td>2001</td>\n",
       "      <td>A. RamosS. HirenzakiS. S. KamalovT. T. S. KuoY...</td>\n",
       "      <td>nucl-th/0101031</td>\n",
       "      <td>matter, nuclear</td>\n",
       "      <td>matter, nuclear</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.222705</td>\n",
       "      <td>0.864732</td>\n",
       "      <td>Extrapolation of K to \\pi\\pi decay amplitude</td>\n",
       "      <td>2001</td>\n",
       "      <td>Mahiko Suzuki</td>\n",
       "      <td>hep-ph/0102028</td>\n",
       "      <td></td>\n",
       "      <td>kaon</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.219966</td>\n",
       "      <td>0.864586</td>\n",
       "      <td>Vector Meson Decay of Baryon Resonances</td>\n",
       "      <td>2001</td>\n",
       "      <td>U. MoselM. Post</td>\n",
       "      <td>nucl-th/0103059</td>\n",
       "      <td>vector</td>\n",
       "      <td>medium, mesons, vector</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.213048</td>\n",
       "      <td>0.864217</td>\n",
       "      <td>Interpretation of SAMPLE and HAPPEX Experiment...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Stanislav DubnickaAnna Zuzana DubnickovaPeter ...</td>\n",
       "      <td>hep-ph/0102171</td>\n",
       "      <td>strange</td>\n",
       "      <td>strange, vector</td>\n",
       "      <td>observation, anti, flow, signal, neutral, stra...</td>\n",
       "      <td>2025-09-18T20:21:34.904999</td>\n",
       "      <td>Motivation, Extends</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank     score    cosine  \\\n",
       "0     0  1.000000  0.906184   \n",
       "1     1  0.359875  0.872047   \n",
       "2     2  0.357546  0.871923   \n",
       "3     3  0.291613  0.868406   \n",
       "4     4  0.277690  0.867664   \n",
       "5     5  0.246621  0.866007   \n",
       "6     6  0.240123  0.865661   \n",
       "7     7  0.222705  0.864732   \n",
       "8     8  0.219966  0.864586   \n",
       "9     9  0.213048  0.864217   \n",
       "\n",
       "                                               title  year  \\\n",
       "0  Anti-flow of K$^0_s$ Mesons in 6 AGeV Au + Au ...  2001   \n",
       "1                  High p_T Higgs signal for the LHC  2001   \n",
       "2  Production and collective behavior of strange ...  2001   \n",
       "3   Polarization Insights for Active Galactic Nuclei  2001   \n",
       "4  Exploring New Physics in the $B\\to \\phi K$ System  2001   \n",
       "5  Asymmetries in phi photoproduction and the OZI...  2001   \n",
       "6       Antikaons in nuclei and dense nuclear matter  2001   \n",
       "7       Extrapolation of K to \\pi\\pi decay amplitude  2001   \n",
       "8            Vector Meson Decay of Baryon Resonances  2001   \n",
       "9  Interpretation of SAMPLE and HAPPEX Experiment...  2001   \n",
       "\n",
       "                                             authors          paper_id  \\\n",
       "0  P. ChungN. N. AjitanandJ. M. AlexanderM. Ander...   nucl-ex/0101003   \n",
       "1                V. A. KhozeA. D. MartinM. G. Ryskin    hep-ph/0104230   \n",
       "2                                      C. Pinkenburg   nucl-ex/0104025   \n",
       "3                                 R. R. J. Antonucci  astro-ph/0103048   \n",
       "4                      Robert FleischerThomas Mannel    hep-ph/0103121   \n",
       "5                             Yongseok OhH. C. Bhang   nucl-th/0104068   \n",
       "6  A. RamosS. HirenzakiS. S. KamalovT. T. S. KuoY...   nucl-th/0101031   \n",
       "7                                      Mahiko Suzuki    hep-ph/0102028   \n",
       "8                                    U. MoselM. Post   nucl-th/0103059   \n",
       "9  Stanislav DubnickaAnna Zuzana DubnickovaPeter ...    hep-ph/0102171   \n",
       "\n",
       "        title_matches                                        abs_matches  \\\n",
       "0  anti, flow, mesons  anti, density, flow, high, kaon, matter, mediu...   \n",
       "1        high, signal                                       anti, signal   \n",
       "2             strange       anti, flow, mesons, neutral, signal, strange   \n",
       "3                                                    high, understanding   \n",
       "4                                                                          \n",
       "5                                                        density, vector   \n",
       "6     matter, nuclear                                    matter, nuclear   \n",
       "7                                                                   kaon   \n",
       "8              vector                             medium, mesons, vector   \n",
       "9             strange                                    strange, vector   \n",
       "\n",
       "                                    query_terms_used  \\\n",
       "0  observation, anti, flow, signal, neutral, stra...   \n",
       "1  observation, anti, flow, signal, neutral, stra...   \n",
       "2  observation, anti, flow, signal, neutral, stra...   \n",
       "3  observation, anti, flow, signal, neutral, stra...   \n",
       "4  observation, anti, flow, signal, neutral, stra...   \n",
       "5  observation, anti, flow, signal, neutral, stra...   \n",
       "6  observation, anti, flow, signal, neutral, stra...   \n",
       "7  observation, anti, flow, signal, neutral, stra...   \n",
       "8  observation, anti, flow, signal, neutral, stra...   \n",
       "9  observation, anti, flow, signal, neutral, stra...   \n",
       "\n",
       "                 retrieved_at classifier_functions  \n",
       "0  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "1  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "2  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "3  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "4  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "5  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "6  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "7  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "8  2025-09-18T20:21:34.904999  Motivation, Extends  \n",
       "9  2025-09-18T20:21:34.904999  Motivation, Extends  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Run llm_test.py to regenerate the file (it overwrites classified_outputs.jsonl)\n",
    "\n",
    "# 2) Process the latest query (query-only retrieval)\n",
    "df_all = process_current_classification(  # <- use the query-only function\n",
    "    classified_path=\"classified_outputs.jsonl\",\n",
    "    out_dir=\"outputs\",\n",
    "    topN=50,\n",
    "    topK_return=20,\n",
    "    normalize_scores=True,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "# 3) Inspect results (no sentence_idx anymore)\n",
    "if df_all.empty:\n",
    "    print(\"No rows saved â€” check your classification or retrieval.\")\n",
    "else:\n",
    "    cols = [\"rank\",\"score\",\"cosine\",\"title\",\"year\",\"authors\",\"paper_id\",\"title_matches\",\"abs_matches\",\"query_terms_used\",\"retrieved_at\",\"classifier_functions\"]\n",
    "    cols = [c for c in cols if c in df_all.columns]  # keep only existing\n",
    "    display(df_all[df_all[\"rank\"].notna()].sort_values(\"rank\")[cols].head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
